{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Sam COLLIN - 4031621\n",
        "\n",
        "This notebook is the main code for the task of incremental learning. The guildelines are the following:\n",
        "\n",
        "After having a look at this paper https://arxiv.org/pdf/1511.05939v2, our goal is to modify it to train an ML model continually on the CIFAR-10 dataset. For this, instead of training the model once on the complete CIFAR-10 dataset, we will divide the dataset into 5 separate sets, with each set containing data for 2 classes. Train the model incrementally on the training data of all the sets in 5 increments. In each increment, train the model on the training set of that increment only and not any previous increments. After training in each increment, evaluate the model on the test set of all the sets used so far. Report the model's training accuracy for 5 increments.\n",
        "\n",
        "- First, I tried different configurations to get good results on the basic training mode using code from the internet. I'll explain the best results I got.\n",
        "\n",
        "- Then, I explain the modified code in cells to adapt it to the incremental learning task.\n"
      ],
      "metadata": {
        "id": "m8-Uyidcg-uh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initial clone"
      ],
      "metadata": {
        "id": "OIqMxauPlMCV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r910EfCS4ojz",
        "outputId": "622869bb-e02b-4556-f8e7-bbad5f869780",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path '/content/continous_magnet' already exists and is not an empty directory.\n",
            "/content/continous_magnet\n",
            "total 44\n",
            "drwxr-xr-x 7 root root  4096 Mar 10 15:05  .\n",
            "drwxr-xr-x 1 root root  4096 Mar 10 15:05  ..\n",
            "drwxr-xr-x 8 root root  4096 Mar 10 15:05  .git\n",
            "drwxr-xr-x 4 root root  4096 Mar 10 15:06  Magnet\n",
            "-rw-r--r-- 1 root root 10308 Mar 10 15:05  Magnet_original.zip\n",
            "drwxr-xr-x 4 root root  4096 Mar 10 15:05 'Magnet plus vmf'\n",
            "-rw-r--r-- 1 root root   538 Mar 10 15:05  Readme.txt\n",
            "drwxr-xr-x 4 root root  4096 Mar 10 15:05  Triplet\n",
            "drwxr-xr-x 2 root root  4096 Mar 10 15:05  VMF\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/continous_magnet/Magnet'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "!git clone https://github.com/GitHubSamm/continous_magnet.git /content/continous_magnet\n",
        "%cd /content/continous_magnet\n",
        "!ls -la\n",
        "os.chdir('/content/continous_magnet/Magnet')\n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Update file (when needed)"
      ],
      "metadata": {
        "id": "T-pETrxElYgD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/continous_magnet\n",
        "os.chdir('/content/')\n",
        "# !git clone https://github.com/GitHubSamm/continous_magnet.git /content/continous_magnet\n",
        "# %cd /content/continous_magnet\n",
        "# !ls -la\n",
        "# os.chdir('/content/continous_magnet/Magnet')\n",
        "# os.getcwd()"
      ],
      "metadata": {
        "id": "xFiX69IFdzEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Result obtained with:\n",
        "\n",
        "$Facnet_config = {\n",
        "    \"Epochs\" : 50,               # Number of epochs\n",
        "    \"learning_rate\" : 10**-4,     # learning rate\n",
        "    \"epsilon\" : 1e-8,             # epsilon to avoid 0 in denum in loss function\n",
        "    \"alpha\" : 1,                  # Margin alpha for magnet loss\n",
        "    \"nb_clusters\" : [20,20,20,20,20,20,20,20,20,20],     # Number of clusters per class\n",
        "    \"M\" : 16,                     # Number of clusters present in a mini-batch\n",
        "    \"D\" : 8,                      # Number of smaples selected from each cluster in the mini-batch\n",
        "    \"K\" : 15,                     # K for K-nearest neighbors\n",
        "    \"L\" : 3,                      # L number of nearest clusters used to predict the label of a given query (instead of KNN: magnet evaluation)\n",
        "    \"nb_batches\" : 30,            \n",
        "    \"list_classes\" : [0,1,2,3,4,5,6,7,8,9],       # List of classes\n",
        "    \"batch_size\" : 32,            # mini batch size to forward the data (out of training)\n",
        "    \"optimizer_flag\" : 'Adam',    # Optimizer\n",
        "    \"width\" : 2,                  # width of wide residual block\n",
        "}$"
      ],
      "metadata": {
        "id": "AXvAyDqRmQbc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/continous_magnet/Magnet/main.py"
      ],
      "metadata": {
        "id": "ClfnJBoUeazO",
        "outputId": "6fc14064-1ccd-4ebf-d7f6-50c9a4b0b80a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Importing data...\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "images_train size: torch.Size([5000, 3, 32, 32])\n",
            "labels_train size: torch.Size([5000])\n",
            "images_test size: torch.Size([1000, 3, 32, 32])\n",
            "labels_test size: torch.Size([1000])\n",
            "   #######################  Train Epoch: 0 train_acc: 0.2872  test_acc: 0.2760 ###################       \n",
            "   #######################  Train Epoch: 1 train_acc: 0.3716  test_acc: 0.3470 ###################       \n",
            "   #######################  Train Epoch: 2 train_acc: 0.4310  test_acc: 0.3820 ###################       \n",
            "   #######################  Train Epoch: 3 train_acc: 0.4690  test_acc: 0.4170 ###################       \n",
            "   #######################  Train Epoch: 4 train_acc: 0.5040  test_acc: 0.4120 ###################       \n",
            "   #######################  Train Epoch: 5 train_acc: 0.5172  test_acc: 0.4280 ###################       \n",
            "   #######################  Train Epoch: 6 train_acc: 0.5614  test_acc: 0.4340 ###################       \n",
            "   #######################  Train Epoch: 7 train_acc: 0.5906  test_acc: 0.4290 ###################       \n",
            "   #######################  Train Epoch: 8 train_acc: 0.6130  test_acc: 0.4550 ###################       \n",
            "   #######################  Train Epoch: 9 train_acc: 0.6332  test_acc: 0.4410 ###################       \n",
            "   #######################  Train Epoch: 10 train_acc: 0.6438  test_acc: 0.4490 ###################       \n",
            "   #######################  Train Epoch: 11 train_acc: 0.6780  test_acc: 0.4320 ###################       \n",
            "   #######################  Train Epoch: 12 train_acc: 0.6984  test_acc: 0.4580 ###################       \n",
            "   #######################  Train Epoch: 13 train_acc: 0.7142  test_acc: 0.4380 ###################       \n",
            "   #######################  Train Epoch: 14 train_acc: 0.7352  test_acc: 0.4510 ###################       \n",
            "   #######################  Train Epoch: 15 train_acc: 0.7430  test_acc: 0.4580 ###################       \n",
            "   #######################  Train Epoch: 16 train_acc: 0.7460  test_acc: 0.4580 ###################       \n",
            "   #######################  Train Epoch: 17 train_acc: 0.7682  test_acc: 0.4390 ###################       \n",
            "   #######################  Train Epoch: 18 train_acc: 0.7666  test_acc: 0.4560 ###################       \n",
            "   #######################  Train Epoch: 19 train_acc: 0.7814  test_acc: 0.4320 ###################       \n",
            "   #######################  Train Epoch: 20 train_acc: 0.7894  test_acc: 0.4510 ###################       \n",
            "   #######################  Train Epoch: 21 train_acc: 0.8092  test_acc: 0.4490 ###################       \n",
            "   #######################  Train Epoch: 22 train_acc: 0.7962  test_acc: 0.4580 ###################       \n",
            "   #######################  Train Epoch: 23 train_acc: 0.8228  test_acc: 0.4640 ###################       \n",
            "   #######################  Train Epoch: 24 train_acc: 0.8408  test_acc: 0.4680 ###################       \n",
            "   #######################  Train Epoch: 25 train_acc: 0.8472  test_acc: 0.4640 ###################       \n",
            "   #######################  Train Epoch: 26 train_acc: 0.8438  test_acc: 0.4600 ###################       \n",
            "   #######################  Train Epoch: 27 train_acc: 0.8566  test_acc: 0.4730 ###################       \n",
            "   #######################  Train Epoch: 28 train_acc: 0.8500  test_acc: 0.4650 ###################       \n",
            "   #######################  Train Epoch: 29 train_acc: 0.8694  test_acc: 0.4510 ###################       \n",
            "   #######################  Train Epoch: 30 train_acc: 0.8844  test_acc: 0.4680 ###################       \n",
            "   #######################  Train Epoch: 31 train_acc: 0.8816  test_acc: 0.4600 ###################       \n",
            "   #######################  Train Epoch: 32 train_acc: 0.8594  test_acc: 0.4660 ###################       \n",
            "Traceback (most recent call last):\n",
            "  File \"/content/continous_magnet/Magnet/main.py\", line 50, in <module>\n",
            "    learning_function(model,optimizer,device,images_train, labels_train,images_test,labels_test)\n",
            "  File \"/content/continous_magnet/Magnet/learning_function.py\", line 62, in learning_function\n",
            "    loss   = loss_function(alpha).forward(output, batch_label,chosen_clusters,device).to(device)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/continous_magnet/Magnet/loss_function.py\", line 37, in forward\n",
            "    dis        = -(1/(2*sigma.pow(2)))*self.pdist(output[s],mean_clusters).pow(2)\n",
            "                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/distance.py\", line 58, in forward\n",
            "    return F.pairwise_distance(x1, x2, self.norm, self.eps, self.keepdim)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Result obtained with:\n",
        "\n",
        "$Facnet_config = {\n",
        "    \"Epochs\" : 50,               # Number of epochs\n",
        "    \"learning_rate\" : 10**-4,     # learning rate\n",
        "    \"epsilon\" : 1e-8,             # epsilon to avoid 0 in denum in loss function\n",
        "    \"alpha\" : 1,                  # Margin alpha for magnet loss\n",
        "    \"nb_clusters\" : [20,20,20,20,20,20,20,20,20,20],     # Number of clusters per class\n",
        "    \"M\" : 16,                     # Number of clusters present in a mini-batch\n",
        "    \"D\" : 8,                      # Number of smaples selected from each cluster in the mini-batch\n",
        "    \"K\" : 15,                     # K for K-nearest neighbors\n",
        "    \"L\" : 128,                      # L number of nearest clusters used to predict the label of a given query (instead of KNN: magnet evaluation)\n",
        "    \"nb_batches\" : 30,            \n",
        "    \"list_classes\" : [0,1,2,3,4,5,6,7,8,9],       # List of classes\n",
        "    \"batch_size\" : 32,            # mini batch size to forward the data (out of training)\n",
        "    \"optimizer_flag\" : 'Adam',    # Optimizer\n",
        "    \"width\" : 2,                  # width of wide residual block\n",
        "}$"
      ],
      "metadata": {
        "id": "xrL7xeyFgYx0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/continous_magnet/Magnet/main.py"
      ],
      "metadata": {
        "id": "C04EeRF0vRds",
        "outputId": "22bedd5c-3a8e-4c45-ef98-a55441a95252",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Importing data...\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "images_train size: torch.Size([20000, 3, 32, 32])\n",
            "labels_train size: torch.Size([20000])\n",
            "images_test size: torch.Size([4000, 3, 32, 32])\n",
            "labels_test size: torch.Size([4000])\n",
            "   #######################  Train Epoch: 0 train_acc: 0.3268  test_acc: 0.3137 ###################       \n",
            "   #######################  Train Epoch: 1 train_acc: 0.4043  test_acc: 0.4017 ###################       \n",
            "   #######################  Train Epoch: 2 train_acc: 0.4351  test_acc: 0.4268 ###################       \n",
            "   #######################  Train Epoch: 3 train_acc: 0.4636  test_acc: 0.4500 ###################       \n",
            "   #######################  Train Epoch: 4 train_acc: 0.4738  test_acc: 0.4640 ###################       \n",
            "   #######################  Train Epoch: 5 train_acc: 0.4948  test_acc: 0.4753 ###################       \n",
            "   #######################  Train Epoch: 6 train_acc: 0.5064  test_acc: 0.4865 ###################       \n",
            "   #######################  Train Epoch: 7 train_acc: 0.5151  test_acc: 0.4968 ###################       \n",
            "   #######################  Train Epoch: 8 train_acc: 0.5249  test_acc: 0.4973 ###################       \n",
            "   #######################  Train Epoch: 9 train_acc: 0.5244  test_acc: 0.4990 ###################       \n",
            "   #######################  Train Epoch: 10 train_acc: 0.5383  test_acc: 0.4990 ###################       \n",
            "   #######################  Train Epoch: 11 train_acc: 0.5382  test_acc: 0.4993 ###################       \n",
            "   #######################  Train Epoch: 12 train_acc: 0.5441  test_acc: 0.5038 ###################       \n",
            "   #######################  Train Epoch: 13 train_acc: 0.5512  test_acc: 0.5100 ###################       \n",
            "   #######################  Train Epoch: 14 train_acc: 0.5591  test_acc: 0.5095 ###################       \n",
            "   #######################  Train Epoch: 15 train_acc: 0.5675  test_acc: 0.5240 ###################       \n",
            "   #######################  Train Epoch: 16 train_acc: 0.5756  test_acc: 0.5202 ###################       \n",
            "   #######################  Train Epoch: 17 train_acc: 0.5728  test_acc: 0.5145 ###################       \n",
            "   #######################  Train Epoch: 18 train_acc: 0.5788  test_acc: 0.5312 ###################       \n",
            "   #######################  Train Epoch: 19 train_acc: 0.5868  test_acc: 0.5212 ###################       \n",
            "   #######################  Train Epoch: 20 train_acc: 0.5965  test_acc: 0.5343 ###################       \n",
            "   #######################  Train Epoch: 21 train_acc: 0.6043  test_acc: 0.5308 ###################       \n",
            "   #######################  Train Epoch: 22 train_acc: 0.6072  test_acc: 0.5280 ###################       \n",
            "   #######################  Train Epoch: 23 train_acc: 0.6093  test_acc: 0.5225 ###################       \n",
            "   #######################  Train Epoch: 24 train_acc: 0.6102  test_acc: 0.5383 ###################       \n",
            "   #######################  Train Epoch: 25 train_acc: 0.6203  test_acc: 0.5327 ###################       \n",
            "   #######################  Train Epoch: 26 train_acc: 0.6227  test_acc: 0.5357 ###################       \n",
            "   #######################  Train Epoch: 27 train_acc: 0.6245  test_acc: 0.5335 ###################       \n",
            "   #######################  Train Epoch: 28 train_acc: 0.6309  test_acc: 0.5393 ###################       \n",
            "   #######################  Train Epoch: 29 train_acc: 0.6404  test_acc: 0.5407 ###################       \n",
            "   #######################  Train Epoch: 30 train_acc: 0.6435  test_acc: 0.5453 ###################       \n",
            "   #######################  Train Epoch: 31 train_acc: 0.6370  test_acc: 0.5390 ###################       \n",
            "   #######################  Train Epoch: 32 train_acc: 0.6511  test_acc: 0.5403 ###################       \n",
            "   #######################  Train Epoch: 33 train_acc: 0.6584  test_acc: 0.5450 ###################       \n",
            "   #######################  Train Epoch: 34 train_acc: 0.6595  test_acc: 0.5423 ###################       \n",
            "   #######################  Train Epoch: 35 train_acc: 0.6650  test_acc: 0.5427 ###################       \n",
            "   #######################  Train Epoch: 36 train_acc: 0.6715  test_acc: 0.5390 ###################       \n",
            "   #######################  Train Epoch: 37 train_acc: 0.6790  test_acc: 0.5420 ###################       \n",
            "   #######################  Train Epoch: 38 train_acc: 0.6783  test_acc: 0.5455 ###################       \n",
            "   #######################  Train Epoch: 39 train_acc: 0.6854  test_acc: 0.5370 ###################       \n",
            "   #######################  Train Epoch: 40 train_acc: 0.6992  test_acc: 0.5427 ###################       \n",
            "   #######################  Train Epoch: 41 train_acc: 0.6977  test_acc: 0.5520 ###################       \n",
            "   #######################  Train Epoch: 42 train_acc: 0.7113  test_acc: 0.5557 ###################       \n",
            "   #######################  Train Epoch: 43 train_acc: 0.7050  test_acc: 0.5423 ###################       \n",
            "   #######################  Train Epoch: 44 train_acc: 0.7089  test_acc: 0.5533 ###################       \n",
            "   #######################  Train Epoch: 45 train_acc: 0.7085  test_acc: 0.5510 ###################       \n",
            "   #######################  Train Epoch: 46 train_acc: 0.7108  test_acc: 0.5493 ###################       \n",
            "   #######################  Train Epoch: 47 train_acc: 0.7228  test_acc: 0.5545 ###################       \n",
            "   #######################  Train Epoch: 48 train_acc: 0.7266  test_acc: 0.5483 ###################       \n",
            "   #######################  Train Epoch: 49 train_acc: 0.7384  test_acc: 0.5553 ###################       \n",
            "   #######################  Train Epoch: 50 train_acc: 0.7356  test_acc: 0.5585 ###################       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's modify the learning function to make it incremental"
      ],
      "metadata": {
        "id": "ZfOlamSwmw72"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CODE\n",
        "\n",
        "First is the configuration: This is where the hyperparameters ared defined. For convenience working with Colab, it is a global dictionnary. For better practice, it should be in a file attached."
      ],
      "metadata": {
        "id": "d4RBnOZc49Mp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Facnet_config = {\n",
        "    \"Epochs\" : 1,                 # Number of epochs\n",
        "    \"learning_rate\" : 10**-4,     # learning rate\n",
        "    \"epsilon\" : 1e-8,             # epsilon to avoid 0 in denum in loss function\n",
        "    \"alpha\" : 1,                  # Margin alpha for magnet loss\n",
        "    \"nb_clusters\" : [20,20,20,20,20,20,20,20,20,20],     # Number of clusters per class\n",
        "    \"M\" : 16,                     # Number of clusters present in a mini-batch\n",
        "    \"D\" : 8,                      # Number of smaples selected from each cluster in the mini-batch\n",
        "    \"K\" : 15,                     # K for K-nearest neighbors\n",
        "    \"L\" : 128,                    # L number of nearest clusters used to predict the label of a given query (instead of KNN: magnet evaluation)\n",
        "    \"nb_batches\" : 30,\n",
        "    \"list_classes\" : [0,1,2,3,4,5,6,7,8,9],       # List of classes\n",
        "    \"batch_size\" : 32,            # mini batch size to forward the data (out of training)\n",
        "    \"optimizer_flag\" : 'Adam',    # Optimizer\n",
        "    \"width\" : 2,                  # width of wide residual block\n",
        "    \"n_increment\" : 5             # # of increments in the learning process\n",
        "}"
      ],
      "metadata": {
        "id": "vkkWnrIn7HTd"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Some functions that will help us.\n",
        "These comes from the code implementation available and are modified to adapt to the project, the environment and the dataset.\n",
        "\n",
        "**create_model**"
      ],
      "metadata": {
        "id": "xBLMrEX25374"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def conv3x3(i_c, o_c, stride=1):\n",
        "    return nn.Conv2d(i_c, o_c, 3, stride, 1, bias=False)\n",
        "\n",
        "\n",
        "class BatchNorm2d(nn.BatchNorm2d):\n",
        "    def __init__(self, channels, momentum=1e-3, eps=1e-3):\n",
        "        super().__init__(channels)\n",
        "        self.update_batch_stats = True\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.update_batch_stats:\n",
        "            return super().forward(x)\n",
        "        else:\n",
        "            return nn.functional.batch_norm(\n",
        "                x, None, None, self.weight, self.bias, True, self.momentum, self.eps\n",
        "            )\n",
        "\n",
        "\n",
        "def relu():\n",
        "    return nn.LeakyReLU(0.1)\n",
        "\n",
        "class residual(nn.Module):\n",
        "    def __init__(self, input_channels, output_channels, stride=1, activate_before_residual=False):\n",
        "        super().__init__()\n",
        "        layer = []\n",
        "        if activate_before_residual:\n",
        "            self.pre_act = nn.Sequential(\n",
        "                #BatchNorm2d(input_channels),\n",
        "                relu()\n",
        "            )\n",
        "        else:\n",
        "            self.pre_act = nn.Identity()\n",
        "            #layer.append(BatchNorm2d(input_channels))\n",
        "            layer.append(relu())\n",
        "        layer.append(conv3x3(input_channels, output_channels, stride))\n",
        "        #layer.append(BatchNorm2d(output_channels))\n",
        "        layer.append(relu())\n",
        "        layer.append(conv3x3(output_channels, output_channels))\n",
        "\n",
        "        if stride >= 2 or input_channels != output_channels:\n",
        "            self.identity = nn.Conv2d(input_channels, output_channels, 1, stride, bias=False)\n",
        "        else:\n",
        "            self.identity = nn.Identity()\n",
        "\n",
        "        self.layer = nn.Sequential(*layer)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pre_act(x)\n",
        "        return self.identity(x) + self.layer(x)\n",
        "\n",
        "class WRN2(nn.Module):\n",
        "    \"\"\" WRN28-width with leaky relu (negative slope is 0.1)\"\"\"\n",
        "    def __init__(self,width):\n",
        "        super().__init__()\n",
        "        self.init_conv = conv3x3(3, 16)\n",
        "\n",
        "        filters = [16, 16*width, 32*width, 64*width]\n",
        "\n",
        "        unit1 = [residual(filters[0], filters[1], activate_before_residual=True)] + [residual(filters[1], filters[1]) for _ in range(1, 4)]\n",
        "        self.unit1 = nn.Sequential(*unit1)\n",
        "\n",
        "        unit2 = [residual(filters[1], filters[2], 2)] + [residual(filters[2], filters[2]) for _ in range(1, 4)]\n",
        "        self.unit2 = nn.Sequential(*unit2)\n",
        "\n",
        "        unit3 = [residual(filters[2], filters[3], 2)] + [residual(filters[3], filters[3]) for _ in range(1, 4)]\n",
        "        self.unit3 = nn.Sequential(*unit3)\n",
        "\n",
        "        #self.unit4 = nn.Sequential(*[BatchNorm2d(filters[3]), relu(), nn.AdaptiveAvgPool2d(1)])\n",
        "        self.unit4 = nn.Sequential(*[relu(), nn.AdaptiveAvgPool2d(1)])\n",
        "        self.dropout1 = torch.nn.Dropout(0.5)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.init_conv(x)\n",
        "        x = self.unit1(x)\n",
        "        x = self.unit2(x)\n",
        "        x = self.unit3(x)\n",
        "        x = self.unit4(x)\n",
        "        if x.shape[0]!=1:\n",
        "            x = torch.squeeze(x)\n",
        "        else:\n",
        "            x = torch.squeeze(x)\n",
        "            x = torch.unsqueeze(x, 0)\n",
        "        x = F.normalize(x, p=2, dim=1)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "Icie_Dee7o8s"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**KNC**<br>\n",
        "Useless remove ?"
      ],
      "metadata": {
        "id": "FfHB4uwe71gu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def KNC(clusters_centers, clusters_labels,test_features,labels_test,sigma):\n",
        "\n",
        "    L = Facnet_config['L']\n",
        "    D  = euclidean_distances(test_features,clusters_centers)\n",
        "    D  = np.square(D)\n",
        "    I  = np.argsort(D,axis=1)\n",
        "    D1 = np.sort(D,axis=1)\n",
        "\n",
        "    D2  = -(1/(2*sigma ))*D1\n",
        "    D2  = np.exp(D1)\n",
        "\n",
        "    label_pred = np.zeros(I.shape[0])\n",
        "    for i in range(I.shape[0]):\n",
        "        cl = clusters_labels[I[i,:L]]\n",
        "        X  = np.zeros(len(Facnet_config['list_classes']))\n",
        "        for j in Facnet_config['list_classes']:\n",
        "            inds = np.where(cl==j)[0]\n",
        "            X[j] = np.sum(D2[i,inds])\n",
        "\n",
        "        label_pred[i] = np.argmax(X/X.sum())\n",
        "\n",
        "    acc = accuracy_score(labels_test, label_pred)\n",
        "    return labels_test,acc\n"
      ],
      "metadata": {
        "id": "KP4g8FFS77VT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**forward_all_images**\n",
        "\n",
        "This function computes representations for all the imgs that are passed (i.e. the two classes for training and all the past classes for testing).\n",
        "It does it minibatch by minibatch (batch_size here is different from the batch created with class_batch). Passes all the images through the model and finally return an array containing the features."
      ],
      "metadata": {
        "id": "r52WzSuT7qiv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def forward_all_images(model,device,all_img):\n",
        "    batch_size          = Facnet_config['batch_size']\n",
        "\n",
        "    device      = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Adding this should handle both numpy array or tensors\n",
        "    # It is useful because CIFAR-10 is imported from torchvision as tensors\n",
        "    # so this step can be avoided\n",
        "    if not isinstance(all_img, torch.Tensor):\n",
        "      all_img = torch.from_numpy(all_img)\n",
        "\n",
        "    all_features  = []\n",
        "    # Iterate batch per batch\n",
        "    for i in range(0,all_img.shape[0],batch_size):\n",
        "        # Check if enough imgs remains to form a complete batch, if not, take the remaining images\n",
        "        if i+batch_size == all_img.shape[0]-1 or i+batch_size > all_img.shape[0]-1:\n",
        "            s         = i\n",
        "            data      = all_img[s:].to(device=device, dtype=torch.float)\n",
        "            features1 = model(data)\n",
        "        else:\n",
        "            s = i\n",
        "            e = s + batch_size\n",
        "\n",
        "            data      = all_img[s:e].to(device=device, dtype=torch.float)\n",
        "            features1 = model(data)\n",
        "\n",
        "        all_features.append(features1.cpu().detach().numpy())\n",
        "\n",
        "    all_features = np.concatenate(all_features,axis=0)\n",
        "\n",
        "    # Return an array containing all the features for each sample\n",
        "    return all_features\n"
      ],
      "metadata": {
        "id": "sDzYLhLR6aIV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**cluster_training**\n",
        "\n",
        "This function extract from the features and the label a set of clusters for each class. It will be useful later to choose the samples that will form the minibatch and compute the loss. It refers to the ``nb_clusters`` hparams to know how much cluster should be created per class.\n",
        "\n",
        "For the project, it has been adapted to manage a specific set of classes (increment)\n",
        "\n",
        "It returns:\n",
        "\n",
        "``clusters_centers``: Where each line is a center of a cluster\n",
        "\n",
        "``clusters_labels``: Each row here is a [c:i] couple indicating class c and cluster i\n",
        "\n",
        "``samples_indices``: Each element is a tab of idx of all the training examples that belong to the cluster at this position\n",
        "\n",
        "``sigma``: The mean variance of all the clusters.\n"
      ],
      "metadata": {
        "id": "aFAEsCzY8boP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "def cluster_training(train_features,train_labels,increment_nb):\n",
        "\n",
        "    # Retrieve the # of clusters needed for the actual class of the increment\n",
        "    nb_clusters_list = Facnet_config['nb_clusters_train']\n",
        "    # Retrieve the class labels that will be considered\n",
        "    list_classes     = Facnet_config['list_classes_train']\n",
        "\n",
        "    clusters_labels  = []\n",
        "    samples_indices  = []\n",
        "    clusters_centers = []\n",
        "    sigma = 0\n",
        "\n",
        "    # For each class among all the one present in the increment\n",
        "    for c in list_classes:\n",
        "\n",
        "        # We retrieve the features belonging to the considered class and the # of clusters associated\n",
        "        nb_clusters          = nb_clusters_list[c-increment_nb*len(nb_clusters_list)]\n",
        "        inds_c               = np.where(train_labels==c)[0]\n",
        "        class_features       = train_features[inds_c]\n",
        "\n",
        "        # We apply KMeans with the wanted # of clusters on the features of this class and add the total inertia for this class to sigma\n",
        "        kM = KMeans(n_clusters=nb_clusters, random_state=0).fit(class_features)\n",
        "        sigma = sigma + kM.inertia_\n",
        "        # From KMeans we retrieve the cluster centers, their labels and the associated idxs of the imgs\n",
        "        for i in range(nb_clusters):\n",
        "            clusters_centers.append(kM.cluster_centers_[i])\n",
        "            clusters_labels.append(np.array([c,i]))\n",
        "            samples_indices.append(inds_c[np.where(kM.labels_==i)[0]])\n",
        "\n",
        "    clusters_centers = np.stack(clusters_centers,axis=0)\n",
        "    clusters_labels  = np.stack(clusters_labels,axis=0)\n",
        "\n",
        "    # Careful here, this assumes that nb_cluster is the same for all the classes\n",
        "    # Sigma is the mean variance for all the clusters\n",
        "    sigma = sigma/(nb_clusters*len(list_classes))\n",
        "\n",
        "    return clusters_centers,clusters_labels,samples_indices,sigma"
      ],
      "metadata": {
        "id": "MdNLvfrR8jMg"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**class_batch**\n",
        "\n",
        "cluster_training gives us a cluster structure. From it, class batch is responsible of creating the mini-batches adapted to the Magnet loss.\n",
        "\n",
        "In the ``initialization``, it retrieve the infos from cluster_training and some hyperparameters. It then construct two matrices, first P_dis is the distance b/w all the cluster centers and the second S_dis is a matrix where each row is all the idxs of the closest neighbors from each sorted according to the computed euclidean distance. The cluster itself has been removed.\n",
        "\n",
        "In the ``construct_batch`` method,"
      ],
      "metadata": {
        "id": "29xVOWnU8iEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "\n",
        "class class_batch():\n",
        "    def __init__(self, clusters_centers,clusters_labels,samples_indices):\n",
        "\n",
        "        # # of neighbors clusters that we take to form the MB\n",
        "        self.M                 = Facnet_config['M']\n",
        "        # # of samples we will use for each cluster\n",
        "        self.D                 = Facnet_config['D']\n",
        "\n",
        "        self.clusters_centers  = clusters_centers\n",
        "        self.clusters_labels   = clusters_labels\n",
        "        self.samples_indices   = samples_indices\n",
        "        self.P_dis             = euclidean_distances(clusters_centers,clusters_centers)\n",
        "        self.S_dis             = np.argsort(self.P_dis,axis=1)[:,1:]\n",
        "\n",
        "    # This function is applied for one cluster at a time (seed). inds1 represents the cluster position b/w all the cluster in the increment.\n",
        "    # samples_indices has the same idxing where each idx is an array of all the indices of the samples belonging to the cluster at pos inds1\n",
        "    def construct_batch(self,inds1):\n",
        "        batch_inds  = []\n",
        "        batch_label = []\n",
        "\n",
        "        # If the seed cluster has less than D samples\n",
        "        if self.samples_indices[inds1].shape[0]<self.D:\n",
        "            # Take all the sample of the seed cluster (label is also retrieved and repeated D times)\n",
        "            batch_inds.append(self.samples_indices[inds1])\n",
        "            batch_label.append(np.repeat(np.expand_dims(self.clusters_labels[inds1],axis=0),self.samples_indices[inds1].shape[0],axis=0))\n",
        "        else:\n",
        "            # Take D random samples of the seed cluster (label is also retrieved and repeated D times)\n",
        "            batch_inds.append(self.samples_indices[inds1][np.random.randint(low=0, high=self.samples_indices[inds1].shape[0], size=self.D)])\n",
        "            batch_label.append(np.repeat(np.expand_dims(self.clusters_labels[inds1],axis=0),self.D,axis=0))\n",
        "\n",
        "        # Determines which are the M closest centroids using  the S_dis matrix\n",
        "        clusters_inds = self.choose_nearest(inds1)\n",
        "\n",
        "        # For each of the chosen closest clusters we also retrieve D samples and labels\n",
        "        for i in range(clusters_inds.shape[0]):\n",
        "            inds2 = clusters_inds[i]\n",
        "            if self.samples_indices[inds2].shape[0]<self.D:\n",
        "                batch_inds.append(self.samples_indices[inds2])\n",
        "                batch_label.append(np.repeat(np.expand_dims(self.clusters_labels[inds2],axis=0),self.samples_indices[inds2].shape[0],axis=0))\n",
        "            else:\n",
        "                batch_inds.append(self.samples_indices[inds2][np.random.randint(low=0, high=self.samples_indices[inds2].shape[0], size=self.D)])\n",
        "                batch_label.append(np.repeat(np.expand_dims(self.clusters_labels[inds2],axis=0),self.D,axis=0))\n",
        "\n",
        "        batch_inds      = np.concatenate(batch_inds,axis=0)\n",
        "        batch_label     = np.concatenate(batch_label,axis=0)\n",
        "        chosen_clusters = np.unique(batch_label, axis=0)\n",
        "\n",
        "        # Returns in form of arrays\n",
        "        return batch_inds,batch_label,chosen_clusters\n",
        "\n",
        "    def choose_nearest(self,inds1):\n",
        "\n",
        "        pos = int((2/3)*self.M)\n",
        "        neg = int((1/3)*self.M)\n",
        "\n",
        "        labels  = self.clusters_labels[self.S_dis[inds1,:],0]\n",
        "        test    = np.where(labels[:self.M]!=self.clusters_labels[inds1,0])[0].shape[0]\n",
        "\n",
        "        # We make sure the M closest neighbors are at least composed of 1/3 of different classes\n",
        "        if test>neg:\n",
        "            clusters_inds = self.S_dis[inds1,:self.M]\n",
        "        else:\n",
        "            inds_eq   = np.where(labels==self.clusters_labels[inds1,0])[0]\n",
        "            inds_ineq = np.where(labels!=self.clusters_labels[inds1,0])[0]\n",
        "\n",
        "            clusters_inds = np.concatenate([inds_eq[:pos],inds_ineq[:neg]],axis=0)\n",
        "\n",
        "        return clusters_inds\n",
        "\n"
      ],
      "metadata": {
        "id": "X8O2tFmY87tB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**test_model**\n",
        "\n",
        "The role of test model is to evaluate the actual representations."
      ],
      "metadata": {
        "id": "V2RKU_1k8bUX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def test_model(train_features, labels_train,test_features,labels_test, train=False):\n",
        "\n",
        "    # Retrieve the # of neighbors to consider\n",
        "    K                   = Facnet_config['K']\n",
        "\n",
        "    # Compute squared distances b/w train and test features\n",
        "    D  = euclidean_distances(test_features,train_features)\n",
        "    D  = np.square(D)\n",
        "\n",
        "    # I here is a matrix where each row is the idx of the closest train sample for each test example (closest first)\n",
        "    I  = np.argsort(D,axis=1)\n",
        "\n",
        "    # Compute the confidence vector with only the considered neighbors (removing itself in case of training evaluation I guess)\n",
        "    Conf1     = KNN_conf(I[:,1:K+1], labels_train, train)\n",
        "    # Looks useless\n",
        "    Conf      = np.max(Conf1,axis=1)\n",
        "\n",
        "    # Prediction and accuracy\n",
        "    label_pred = np.argmax(Conf1,axis=1)\n",
        "    acc = accuracy_score(labels_test, label_pred)\n",
        "\n",
        "    return label_pred,acc\n",
        "\n",
        "\n",
        "def KNN_conf(mat, labels_train, train):\n",
        "\n",
        "    # Make sure we evalueate on the considered classes defined by the increment\n",
        "    if train:\n",
        "      # Create a confidence vector that is the fraction of the K neighbors that belongs to the class (each column is a class of the increment)\n",
        "      conf_vector1 = np.zeros((mat.shape[0],len(Facnet_config['list_classes_train'])))\n",
        "      for i in range(mat.shape[0]):\n",
        "          for c,j in enumerate(Facnet_config['list_classes_train']):\n",
        "              conf_vector1[i,c] = np.where(labels_train[mat[i,:]]==j)[0].shape[0]/mat.shape[1]\n",
        "\n",
        "    else:\n",
        "      conf_vector1 = np.zeros((mat.shape[0],len(Facnet_config['list_classes_test'])))\n",
        "      for i in range(mat.shape[0]):\n",
        "          for c,j in enumerate(Facnet_config['list_classes_test']):\n",
        "              conf_vector1[i,c] = np.where(labels_train[mat[i,:]]==j)[0].shape[0]/mat.shape[1]\n",
        "\n",
        "    return conf_vector1"
      ],
      "metadata": {
        "id": "8AjjQSor9JDn"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**resize_data**"
      ],
      "metadata": {
        "id": "AldXN3XG9UwB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def resize_dataset(dataset, n_samples, n_cls=10):\n",
        "    \"\"\"This function aims to resize the CIFAR-10 dataset with a given number\n",
        "    of samples per class.\n",
        "    It assumes that the labels are digits from 0 to n_class\"\"\"\n",
        "    imgs_per_class = [[] for i in range(n_cls)]\n",
        "    labels_per_class = [[] for i in range(n_cls)]\n",
        "\n",
        "    for img, label in dataset:\n",
        "        if(len(imgs_per_class[label]) < n_samples):\n",
        "            imgs_per_class[label].append(img)\n",
        "            labels_per_class[label].append(label)\n",
        "\n",
        "    imgs_per_class = [torch.stack(imgs) for imgs in imgs_per_class]\n",
        "    labels_per_class = [torch.tensor(labels) for labels in labels_per_class]\n",
        "\n",
        "    images_set = torch.cat(imgs_per_class)\n",
        "    labels_set = torch.cat(labels_per_class)\n",
        "\n",
        "    return images_set, labels_set\n",
        "\n",
        "def split_train_data(train_data, train_labels, n_increments):\n",
        "    \"\"\"This function is used to split the training data into equally sized increments regarding the # of classes in it.\n",
        "        It returns a list with all train set corresponding to the increments\"\"\"\n",
        "    train_data_per_increment = []\n",
        "    train_label_per_increment = []\n",
        "    n_cls = len(torch.unique(train_labels))\n",
        "    n_class_per_increment = n_cls // n_increments\n",
        "\n",
        "    # Make sure the increment number can divide properly the total number of classes\n",
        "    if n_cls % n_increments != 0:\n",
        "        raise ValueError(\"n_increments should divide the number of classes in the dataset\")\n",
        "\n",
        "    # Assuming class labels goes from 0 to n_classes\n",
        "    # Retrieve the portion of the main set an labels that belongs to the considered classes of the increment\n",
        "    for incr in range(n_increments):\n",
        "        cls_idx = torch.tensor([i+incr*n_class_per_increment for i in range(n_class_per_increment)])\n",
        "        incr_idx = torch.isin(train_labels, cls_idx)\n",
        "        train_label_per_increment.append(train_labels[incr_idx])\n",
        "        train_data_per_increment.append(train_data[incr_idx])\n",
        "\n",
        "    return train_data_per_increment, train_label_per_increment\n",
        "\n",
        "def split_test_data(test_data, test_labels, n_increments):\n",
        "  \"\"\"This function is used to split the test data into set of increasing size gathering all the past data.\n",
        "      It returns two list of data and labels associated with each increment\"\"\"\n",
        "  test_data_per_increment = []\n",
        "  test_label_per_increment = []\n",
        "\n",
        "  n_cls = len(torch.unique(test_labels))\n",
        "  n_class_per_increment = n_cls // n_increments\n",
        "\n",
        "  if n_cls % n_increments != 0:\n",
        "        raise ValueError(\"n_increments should divide the number of classes in the dataset\")\n",
        "\n",
        "  for incr in range(1, n_increments+1):\n",
        "    cls_idx = torch.tensor([i for i in range(n_class_per_increment * incr)])\n",
        "    incr_idx = torch.isin(test_labels, cls_idx)\n",
        "    test_label_per_increment.append(test_labels[incr_idx])\n",
        "    test_data_per_increment.append(test_data[incr_idx])\n",
        "\n",
        "  return test_data_per_increment, test_label_per_increment\n"
      ],
      "metadata": {
        "id": "V_EtYbkS9YU3"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**loss_function**"
      ],
      "metadata": {
        "id": "_p1pjmz29iux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.autograd import Function\n",
        "\n",
        "class loss_function(Function):\n",
        "    def __init__(self, alpha):\n",
        "        self.alpha  = alpha\n",
        "        self.M      = Facnet_config['M']\n",
        "        self.D      = Facnet_config['D']\n",
        "        self.pdist  = torch.nn.PairwiseDistance(p=2)\n",
        "\n",
        "        self.epsilon      = Facnet_config['epsilon']\n",
        "\n",
        "    def mean_sigma(self, output, batch_label,chosen_clusters,device):\n",
        "\n",
        "        sigma         = torch.zeros(chosen_clusters.shape[0]).to(device)\n",
        "        mean_clusters = torch.zeros((chosen_clusters.shape[0],output.shape[1])).to(device)\n",
        "        for i in range(chosen_clusters.shape[0]):\n",
        "            inds               = torch.where((batch_label[:,0]==chosen_clusters[i,0]) & (batch_label[:,1]==chosen_clusters[i,1]) )[0]\n",
        "            mean_clusters[i,:] = output[inds].mean(0)\n",
        "            sigma[i]           = self.pdist(output[inds].mean(0),output[inds]).pow(2).mean(0)\n",
        "\n",
        "        sigma = sigma.mean(0)\n",
        "        return sigma, mean_clusters\n",
        "\n",
        "    def forward(self, output, batch_label,chosen_clusters,device):\n",
        "\n",
        "        sigma, mean_clusters = self.mean_sigma(output, batch_label,chosen_clusters,device)\n",
        "        loss1 = torch.zeros(output.shape[0]).to(device)\n",
        "        for s in range(output.shape[0]):\n",
        "            dis        = -(1/(2*sigma.pow(2)))*self.pdist(output[s],mean_clusters).pow(2)\n",
        "            inds1      = torch.where((chosen_clusters[:,0]==batch_label[s,0]) & (chosen_clusters[:,1]==batch_label[s,1]) )[0]\n",
        "            inds2      = torch.where(chosen_clusters[:,0]!=batch_label[s,0])[0]\n",
        "\n",
        "            num        = torch.exp(dis[inds1]-self.alpha)\n",
        "            den        = torch.exp(dis[inds2]).sum(0)\n",
        "            loss1[s]   = -torch.log(num/(den+self.epsilon) + self.epsilon)\n",
        "\n",
        "        loss2 = torch.clamp(loss1, min = 0.0)\n",
        "        loss  = loss2.mean(0)\n",
        "\n",
        "        return loss\n"
      ],
      "metadata": {
        "id": "zumv80GH9mTO"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from forward_all_images import forward_all_images\n",
        "#from loss_function import loss_function\n",
        "#from test_model import test_model\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "#from config import Facnet_config\n",
        "#from cluster_training import cluster_training\n",
        "#from class_batch import class_batch\n",
        "\n",
        "def incremental_learning_function(model,optimizer,device,train_data_per_increment, train_label_per_increment,test_data_per_increment,test_label_per_increment, n_increment):\n",
        "\n",
        "\n",
        "    ''' #################################################  initialization  ################################################### '''\n",
        "    Epochs              = Facnet_config['Epochs']\n",
        "    nb_batches          = Facnet_config['nb_batches']\n",
        "    alpha               = Facnet_config['alpha']\n",
        "    list_cls = Facnet_config['list_classes']\n",
        "    nb_clstr = Facnet_config['nb_clusters']\n",
        "\n",
        "    train_acc_increment, test_acc_increment = [], []\n",
        "\n",
        "    # iterate over each increment\n",
        "    for increment in range(n_increment):\n",
        "\n",
        "      # # of classes in one increment\n",
        "      increment_size = len(list_cls) // n_increment\n",
        "\n",
        "      # Defines the new classes that are observed by the model at each increment for training hiding previous one\n",
        "      list_cls_train_incr = list_cls[increment_size*increment:increment_size*(increment+1)]\n",
        "      # Defines the new classes that are observed by the model at each increment for testing keeping previous one\n",
        "      list_cls_test_incr = list_cls[0:increment_size*(increment+1)]\n",
        "      Facnet_config['list_classes_train'] = list_cls_train_incr\n",
        "      Facnet_config['list_classes_test'] = list_cls_test_incr\n",
        "\n",
        "      # Retrieve # of clusters for the considered class\n",
        "      nb_cluster_train_incr = nb_clstr[increment_size*increment:increment_size*(increment+1)]\n",
        "      Facnet_config['nb_clusters_train'] = nb_cluster_train_incr\n",
        "\n",
        "      train_acc, test_acc, loss = [], [], []\n",
        "\n",
        "      print(\"----------------------\\n\"\n",
        "      +f\"Starting training increment {increment+1} on classes {list_cls_train_incr} and testing on classes {list_cls_test_incr} ...\\n\"\n",
        "      +\"----------------------\")\n",
        "\n",
        "      # Retrieve the test/train data and labels associated to this increment\n",
        "      images_train, labels_train = train_data_per_increment[increment], train_label_per_increment[increment]\n",
        "      images_test, labels_test = test_data_per_increment[increment], test_label_per_increment[increment]\n",
        "\n",
        "      ''' #################################################  test the initial model  ################################################### '''\n",
        "      # Compute features\n",
        "      train_features = forward_all_images(model,device,images_train)\n",
        "      test_features  = forward_all_images(model,device,images_test)\n",
        "\n",
        "      # Use KMeans and compute the cluster centers, labels, idx associated with each and the mean variance sigma\n",
        "      clusters_centers,clusters_labels,samples_indices,sigma = cluster_training(train_features,labels_train, increment_nb=increment)\n",
        "      # Create a mini batch object from the info of the clustering\n",
        "      CB      = class_batch(clusters_centers,clusters_labels,samples_indices)\n",
        "\n",
        "      # Evaluate the model first without training\n",
        "      _,train_acc_1  = test_model(train_features, labels_train,train_features,labels_train, train=True)\n",
        "      _,test_acc_1   = test_model(train_features, labels_train,test_features,labels_test, train=False)\n",
        "\n",
        "      train_acc.append(train_acc_1)\n",
        "      test_acc.append(test_acc_1)\n",
        "\n",
        "      torch.save(model.state_dict(), r'models\\model'+str(0)+'.pth')\n",
        "      print(\"   #######################  Train Epoch: {} train_acc: {:0.4f}  test_acc: {:0.4f} ###################       \".format(0,train_acc[-1],test_acc[-1]))\n",
        "\n",
        "      ''' #################################################### block for learning  ########################################################## '''\n",
        "      for i in range(1,Epochs+1):\n",
        "\n",
        "          '''##################################################### Learning ################################################################################'''\n",
        "\n",
        "\n",
        "          indices = np.arange(clusters_centers.shape[0])\n",
        "          # For each cluster\n",
        "          for s in range(indices.shape[0]):\n",
        "            # we create a mini_batch and retrieve the idxs of the chosen examples, labels, and neigbors clusters\n",
        "              batch_inds,batch_label,chosen_clusters = CB.construct_batch(indices[s])\n",
        "              targets                = images_train[batch_inds]\n",
        "\n",
        "              # Once again, I just modified to handle tensors\n",
        "              if isinstance(targets, torch.Tensor):\n",
        "                targets = targets.to(device=device, dtype=torch.float)\n",
        "              else:\n",
        "                targets         = torch.from_numpy(targets).to(device=device, dtype=torch.float)\n",
        "\n",
        "              batch_label     = torch.from_numpy(batch_label).to(device=device, dtype=torch.float)\n",
        "              chosen_clusters = torch.from_numpy(chosen_clusters).to(device=device, dtype=torch.float)\n",
        "\n",
        "              # Train and update the model\n",
        "              model.train()\n",
        "              output  = model(targets)\n",
        "              loss   = loss_function(alpha).forward(output, batch_label,chosen_clusters,device).to(device)\n",
        "\n",
        "              optimizer.zero_grad()\n",
        "              loss.backward()\n",
        "              optimizer.step()\n",
        "\n",
        "\n",
        "          '''######################################################## Forward the data  ########################################################################'''\n",
        "          # After an update for each cluster we pass the imgs again through the model\n",
        "          train_features = forward_all_images(model,device,images_train)\n",
        "          test_features  = forward_all_images(model,device,images_test)\n",
        "\n",
        "          '''########################################################## Clustering  ##########################################################################'''\n",
        "          # Defined the structure again using KMeans\n",
        "          clusters_centers,clusters_labels,samples_indices,sigma = cluster_training(train_features,labels_train, increment_nb=increment)\n",
        "          CB      = class_batch(clusters_centers,clusters_labels,samples_indices)\n",
        "\n",
        "          '''##################################################### testing with KNN ##########################################################################'''\n",
        "          # Re-evaluate\n",
        "          _,train_acc_1  = test_model(train_features, labels_train,train_features,labels_train)\n",
        "          _,test_acc_1   = test_model(train_features, labels_train,test_features,labels_test)\n",
        "\n",
        "          '''###################################################################################################################################################'''\n",
        "          train_acc.append(train_acc_1)\n",
        "          test_acc.append(test_acc_1)\n",
        "\n",
        "          torch.save(model.state_dict(), r'models\\model'+str(i)+'.pth')\n",
        "          print(\"   #######################  Train Epoch: {} train_acc: {:0.4f}  test_acc: {:0.4f} ###################       \".format(i,train_acc[-1],test_acc[-1]))\n",
        "\n",
        "      train_acc = np.stack(train_acc)\n",
        "      test_acc  = np.stack(test_acc)\n",
        "\n",
        "      train_acc_increment.append(train_acc[-1])\n",
        "      test_acc_increment.append(test_acc[-1])\n",
        "\n",
        "      print(f\"Final Accs for increment {increment+1}: Train Acc: {train_acc_increment[increment]:0.4f} Test Acc: {test_acc_increment[increment]:0.4f}\")\n",
        "\n",
        "      np.save(r'data1\\train_acc',train_acc)\n",
        "      np.save(r'data1\\test_acc',test_acc)\n",
        "\n",
        "    train_acc_increment = np.stack(train_acc_increment)\n",
        "    test_acc_increment  = np.stack(test_acc_increment)\n",
        "\n",
        "    np.save(r'data1\\train_acc_per_increment',train_acc_increment)\n",
        "    np.save(r'data1\\test_acc_per_increment',test_acc_increment)"
      ],
      "metadata": {
        "id": "ox7LyG2iQUEJ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main fn"
      ],
      "metadata": {
        "id": "F1Dh3sCqnSsH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "torch.manual_seed(3)\n",
        "#from resize_data import resize_dataset, split_train_data, split_test_data\n",
        "\n",
        "#import create_model\n",
        "#from learning_function import learning_function\n",
        "#from config import Facnet_config\n",
        "\n",
        "if not os.path.exists(\"models\"):\n",
        "    os.makedirs(\"models\")\n",
        "\n",
        "print(\"Importing data...\")\n",
        "# Define a transform (here no resizing is needed for 32x32 images)\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Download and load the CIFAR-10 training and test datasets\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset  = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Use 500 samples per class for learning\n",
        "images_train, labels_train = resize_dataset(train_dataset, 500, 10)\n",
        "images_test, labels_test   = resize_dataset(test_dataset, 100, 10)\n",
        "\n",
        "print(f\"images_train size: {images_train.shape}\")\n",
        "print(f\"labels_train size: {labels_train.shape}\")\n",
        "print(f\"images_test size: {images_test.shape}\")\n",
        "print(f\"labels_test size: {labels_test.shape}\")\n",
        "\n",
        "learning_rate       = Facnet_config['learning_rate']\n",
        "width               = Facnet_config['width']\n",
        "n_increment         = Facnet_config['n_increment']\n",
        "\n",
        "# Split our train/test data in 5 increments\n",
        "train_data_per_increment, train_label_per_increment = split_train_data(images_train, labels_train, n_increment)\n",
        "test_data_per_increment, test_label_per_increment = split_test_data(images_test, labels_test, n_increment)\n",
        "\n",
        "# Create the model\n",
        "model     = WRN2(width)\n",
        "device    = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model     = model.to(device=device, dtype=torch.float)\n",
        "model.eval()\n",
        "\n",
        "# Define the optimizer and start the incremental learning process\n",
        "optimizer     = torch.optim.Adam(model.parameters(),lr = learning_rate)\n",
        "incremental_learning_function(model,optimizer,device,train_data_per_increment, train_label_per_increment,test_data_per_increment,test_label_per_increment,n_increment)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qs5BK5YfnR79",
        "outputId": "237c9318-881f-47b5-a08c-55dfb704f406"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Importing data...\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "images_train size: torch.Size([5000, 3, 32, 32])\n",
            "labels_train size: torch.Size([5000])\n",
            "images_test size: torch.Size([1000, 3, 32, 32])\n",
            "labels_test size: torch.Size([1000])\n",
            "----------------------\n",
            "Starting training increment 1 on classes [0, 1] and testing on classes [0, 1] ...\n",
            "----------------------\n",
            "   #######################  Train Epoch: 0 train_acc: 0.7820  test_acc: 0.7450 ###################       \n",
            "   #######################  Train Epoch: 1 train_acc: 0.7990  test_acc: 0.7550 ###################       \n",
            "Final Accs for increment 1: Train Acc: 0.7990 Test Acc: 0.7550\n",
            "----------------------\n",
            "Starting training increment 2 on classes [2, 3] and testing on classes [0, 1, 2, 3] ...\n",
            "----------------------\n",
            "   #######################  Train Epoch: 0 train_acc: 0.7200  test_acc: 0.3300 ###################       \n",
            "   #######################  Train Epoch: 1 train_acc: 0.7260  test_acc: 0.3250 ###################       \n",
            "Final Accs for increment 2: Train Acc: 0.7260 Test Acc: 0.3250\n",
            "----------------------\n",
            "Starting training increment 3 on classes [4, 5] and testing on classes [0, 1, 2, 3, 4, 5] ...\n",
            "----------------------\n",
            "   #######################  Train Epoch: 0 train_acc: 0.7120  test_acc: 0.2267 ###################       \n",
            "   #######################  Train Epoch: 1 train_acc: 0.7600  test_acc: 0.2333 ###################       \n",
            "Final Accs for increment 3: Train Acc: 0.7600 Test Acc: 0.2333\n",
            "----------------------\n",
            "Starting training increment 4 on classes [6, 7] and testing on classes [0, 1, 2, 3, 4, 5, 6, 7] ...\n",
            "----------------------\n",
            "   #######################  Train Epoch: 0 train_acc: 0.7120  test_acc: 0.1825 ###################       \n",
            "   #######################  Train Epoch: 1 train_acc: 0.7820  test_acc: 0.1862 ###################       \n",
            "Final Accs for increment 4: Train Acc: 0.7820 Test Acc: 0.1862\n",
            "----------------------\n",
            "Starting training increment 5 on classes [8, 9] and testing on classes [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] ...\n",
            "----------------------\n",
            "   #######################  Train Epoch: 0 train_acc: 0.7630  test_acc: 0.1590 ###################       \n",
            "   #######################  Train Epoch: 1 train_acc: 0.8330  test_acc: 0.1710 ###################       \n",
            "Final Accs for increment 5: Train Acc: 0.8330 Test Acc: 0.1710\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DFF0s2X6vyiy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}