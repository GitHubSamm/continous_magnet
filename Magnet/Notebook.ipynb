{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Sam COLLIN - 4031621\n",
        "\n",
        "This notebook is the main code for the task of incremental learning. The guildelines are the following:\n",
        "\n",
        "After having a look at this paper https://arxiv.org/pdf/1511.05939v2, our goal is to modify it to train an ML model continually on the CIFAR-10 dataset. For this, instead of training the model once on the complete CIFAR-10 dataset, we will divide the dataset into 5 separate sets, with each set containing data for 2 classes. Train the model incrementally on the training data of all the sets in 5 increments. In each increment, train the model on the training set of that increment only and not any previous increments. After training in each increment, evaluate the model on the test set of all the sets used so far. Report the model's training accuracy for 5 increments.\n",
        "\n",
        "- First, I tried different configurations to get good results on the basic training mode using code from the internet. I'll explain the best results I got.\n",
        "\n",
        "- Then, I explain the modified code in cells to adapt it to the incremental learning task.\n"
      ],
      "metadata": {
        "id": "m8-Uyidcg-uh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initial clone"
      ],
      "metadata": {
        "id": "OIqMxauPlMCV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r910EfCS4ojz",
        "outputId": "622869bb-e02b-4556-f8e7-bbad5f869780",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path '/content/continous_magnet' already exists and is not an empty directory.\n",
            "/content/continous_magnet\n",
            "total 44\n",
            "drwxr-xr-x 7 root root  4096 Mar 10 15:05  .\n",
            "drwxr-xr-x 1 root root  4096 Mar 10 15:05  ..\n",
            "drwxr-xr-x 8 root root  4096 Mar 10 15:05  .git\n",
            "drwxr-xr-x 4 root root  4096 Mar 10 15:06  Magnet\n",
            "-rw-r--r-- 1 root root 10308 Mar 10 15:05  Magnet_original.zip\n",
            "drwxr-xr-x 4 root root  4096 Mar 10 15:05 'Magnet plus vmf'\n",
            "-rw-r--r-- 1 root root   538 Mar 10 15:05  Readme.txt\n",
            "drwxr-xr-x 4 root root  4096 Mar 10 15:05  Triplet\n",
            "drwxr-xr-x 2 root root  4096 Mar 10 15:05  VMF\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/continous_magnet/Magnet'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "!git clone https://github.com/GitHubSamm/continous_magnet.git /content/continous_magnet\n",
        "%cd /content/continous_magnet\n",
        "!ls -la\n",
        "os.chdir('/content/continous_magnet/Magnet')\n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Update associated files (when needed)"
      ],
      "metadata": {
        "id": "T-pETrxElYgD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/continous_magnet\n",
        "os.chdir('/content/')\n",
        "# !git clone https://github.com/GitHubSamm/continous_magnet.git /content/continous_magnet\n",
        "# %cd /content/continous_magnet\n",
        "# !ls -la\n",
        "# os.chdir('/content/continous_magnet/Magnet')\n",
        "# os.getcwd()"
      ],
      "metadata": {
        "id": "xFiX69IFdzEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1 - classic training\n",
        "I first tried the architecture on regular training format.\n",
        "\n",
        "Result obtained with 500 examples per class and the folllowing hparams:\n",
        "\n",
        "$Facnet_config = {\n",
        "    \"Epochs\" : 50,               # Number of epochs\n",
        "    \"learning_rate\" : 10**-4,     # learning rate\n",
        "    \"epsilon\" : 1e-8,             # epsilon to avoid 0 in denum in loss function\n",
        "    \"alpha\" : 1,                  # Margin alpha for magnet loss\n",
        "    \"nb_clusters\" : [20,20,20,20,20,20,20,20,20,20],     # Number of clusters per class\n",
        "    \"M\" : 16,                     # Number of clusters present in a mini-batch\n",
        "    \"D\" : 8,                      # Number of smaples selected from each cluster in the mini-batch\n",
        "    \"K\" : 15,                     # K for K-nearest neighbors\n",
        "    \"L\" : 3,                      # L number of nearest clusters used to predict the label of a given query (instead of KNN: magnet evaluation)\n",
        "    \"nb_batches\" : 30,            \n",
        "    \"list_classes\" : [0,1,2,3,4,5,6,7,8,9],       # List of classes\n",
        "    \"batch_size\" : 32,            # mini batch size to forward the data (out of training)\n",
        "    \"optimizer_flag\" : 'Adam',    # Optimizer\n",
        "    \"width\" : 2,                  # width of wide residual block\n",
        "}$"
      ],
      "metadata": {
        "id": "AXvAyDqRmQbc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!python /content/continous_magnet/Magnet/main.py"
      ],
      "metadata": {
        "id": "ClfnJBoUeazO",
        "outputId": "6fc14064-1ccd-4ebf-d7f6-50c9a4b0b80a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Importing data...\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "images_train size: torch.Size([5000, 3, 32, 32])\n",
            "labels_train size: torch.Size([5000])\n",
            "images_test size: torch.Size([1000, 3, 32, 32])\n",
            "labels_test size: torch.Size([1000])\n",
            "   #######################  Train Epoch: 0 train_acc: 0.2872  test_acc: 0.2760 ###################       \n",
            "   #######################  Train Epoch: 1 train_acc: 0.3716  test_acc: 0.3470 ###################       \n",
            "   #######################  Train Epoch: 2 train_acc: 0.4310  test_acc: 0.3820 ###################       \n",
            "   #######################  Train Epoch: 3 train_acc: 0.4690  test_acc: 0.4170 ###################       \n",
            "   #######################  Train Epoch: 4 train_acc: 0.5040  test_acc: 0.4120 ###################       \n",
            "   #######################  Train Epoch: 5 train_acc: 0.5172  test_acc: 0.4280 ###################       \n",
            "   #######################  Train Epoch: 6 train_acc: 0.5614  test_acc: 0.4340 ###################       \n",
            "   #######################  Train Epoch: 7 train_acc: 0.5906  test_acc: 0.4290 ###################       \n",
            "   #######################  Train Epoch: 8 train_acc: 0.6130  test_acc: 0.4550 ###################       \n",
            "   #######################  Train Epoch: 9 train_acc: 0.6332  test_acc: 0.4410 ###################       \n",
            "   #######################  Train Epoch: 10 train_acc: 0.6438  test_acc: 0.4490 ###################       \n",
            "   #######################  Train Epoch: 11 train_acc: 0.6780  test_acc: 0.4320 ###################       \n",
            "   #######################  Train Epoch: 12 train_acc: 0.6984  test_acc: 0.4580 ###################       \n",
            "   #######################  Train Epoch: 13 train_acc: 0.7142  test_acc: 0.4380 ###################       \n",
            "   #######################  Train Epoch: 14 train_acc: 0.7352  test_acc: 0.4510 ###################       \n",
            "   #######################  Train Epoch: 15 train_acc: 0.7430  test_acc: 0.4580 ###################       \n",
            "   #######################  Train Epoch: 16 train_acc: 0.7460  test_acc: 0.4580 ###################       \n",
            "   #######################  Train Epoch: 17 train_acc: 0.7682  test_acc: 0.4390 ###################       \n",
            "   #######################  Train Epoch: 18 train_acc: 0.7666  test_acc: 0.4560 ###################       \n",
            "   #######################  Train Epoch: 19 train_acc: 0.7814  test_acc: 0.4320 ###################       \n",
            "   #######################  Train Epoch: 20 train_acc: 0.7894  test_acc: 0.4510 ###################       \n",
            "   #######################  Train Epoch: 21 train_acc: 0.8092  test_acc: 0.4490 ###################       \n",
            "   #######################  Train Epoch: 22 train_acc: 0.7962  test_acc: 0.4580 ###################       \n",
            "   #######################  Train Epoch: 23 train_acc: 0.8228  test_acc: 0.4640 ###################       \n",
            "   #######################  Train Epoch: 24 train_acc: 0.8408  test_acc: 0.4680 ###################       \n",
            "   #######################  Train Epoch: 25 train_acc: 0.8472  test_acc: 0.4640 ###################       \n",
            "   #######################  Train Epoch: 26 train_acc: 0.8438  test_acc: 0.4600 ###################       \n",
            "   #######################  Train Epoch: 27 train_acc: 0.8566  test_acc: 0.4730 ###################       \n",
            "   #######################  Train Epoch: 28 train_acc: 0.8500  test_acc: 0.4650 ###################       \n",
            "   #######################  Train Epoch: 29 train_acc: 0.8694  test_acc: 0.4510 ###################       \n",
            "   #######################  Train Epoch: 30 train_acc: 0.8844  test_acc: 0.4680 ###################       \n",
            "   #######################  Train Epoch: 31 train_acc: 0.8816  test_acc: 0.4600 ###################       \n",
            "   #######################  Train Epoch: 32 train_acc: 0.8594  test_acc: 0.4660 ###################       \n",
            "Traceback (most recent call last):\n",
            "  File \"/content/continous_magnet/Magnet/main.py\", line 50, in <module>\n",
            "    learning_function(model,optimizer,device,images_train, labels_train,images_test,labels_test)\n",
            "  File \"/content/continous_magnet/Magnet/learning_function.py\", line 62, in learning_function\n",
            "    loss   = loss_function(alpha).forward(output, batch_label,chosen_clusters,device).to(device)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/continous_magnet/Magnet/loss_function.py\", line 37, in forward\n",
            "    dis        = -(1/(2*sigma.pow(2)))*self.pdist(output[s],mean_clusters).pow(2)\n",
            "                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/distance.py\", line 58, in forward\n",
            "    return F.pairwise_distance(x1, x2, self.norm, self.eps, self.keepdim)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model overfits, I tried using more samples and it is better but still overfitting at some points.\n",
        "\n",
        "Result obtained with:\n",
        "\n",
        "$Facnet_config = {\n",
        "    \"Epochs\" : 50,               # Number of epochs\n",
        "    \"learning_rate\" : 10**-4,     # learning rate\n",
        "    \"epsilon\" : 1e-8,             # epsilon to avoid 0 in denum in loss function\n",
        "    \"alpha\" : 1,                  # Margin alpha for magnet loss\n",
        "    \"nb_clusters\" : [20,20,20,20,20,20,20,20,20,20],     # Number of clusters per class\n",
        "    \"M\" : 16,                     # Number of clusters present in a mini-batch\n",
        "    \"D\" : 8,                      # Number of smaples selected from each cluster in the mini-batch\n",
        "    \"K\" : 15,                     # K for K-nearest neighbors\n",
        "    \"L\" : 128,                      # L number of nearest clusters used to predict the label of a given query (instead of KNN: magnet evaluation)\n",
        "    \"nb_batches\" : 30,            \n",
        "    \"list_classes\" : [0,1,2,3,4,5,6,7,8,9],       # List of classes\n",
        "    \"batch_size\" : 32,            # mini batch size to forward the data (out of training)\n",
        "    \"optimizer_flag\" : 'Adam',    # Optimizer\n",
        "    \"width\" : 2,                  # width of wide residual block\n",
        "}$"
      ],
      "metadata": {
        "id": "xrL7xeyFgYx0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/continous_magnet/Magnet/main.py"
      ],
      "metadata": {
        "id": "C04EeRF0vRds",
        "outputId": "22bedd5c-3a8e-4c45-ef98-a55441a95252",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Importing data...\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "images_train size: torch.Size([20000, 3, 32, 32])\n",
            "labels_train size: torch.Size([20000])\n",
            "images_test size: torch.Size([4000, 3, 32, 32])\n",
            "labels_test size: torch.Size([4000])\n",
            "   #######################  Train Epoch: 0 train_acc: 0.3268  test_acc: 0.3137 ###################       \n",
            "   #######################  Train Epoch: 1 train_acc: 0.4043  test_acc: 0.4017 ###################       \n",
            "   #######################  Train Epoch: 2 train_acc: 0.4351  test_acc: 0.4268 ###################       \n",
            "   #######################  Train Epoch: 3 train_acc: 0.4636  test_acc: 0.4500 ###################       \n",
            "   #######################  Train Epoch: 4 train_acc: 0.4738  test_acc: 0.4640 ###################       \n",
            "   #######################  Train Epoch: 5 train_acc: 0.4948  test_acc: 0.4753 ###################       \n",
            "   #######################  Train Epoch: 6 train_acc: 0.5064  test_acc: 0.4865 ###################       \n",
            "   #######################  Train Epoch: 7 train_acc: 0.5151  test_acc: 0.4968 ###################       \n",
            "   #######################  Train Epoch: 8 train_acc: 0.5249  test_acc: 0.4973 ###################       \n",
            "   #######################  Train Epoch: 9 train_acc: 0.5244  test_acc: 0.4990 ###################       \n",
            "   #######################  Train Epoch: 10 train_acc: 0.5383  test_acc: 0.4990 ###################       \n",
            "   #######################  Train Epoch: 11 train_acc: 0.5382  test_acc: 0.4993 ###################       \n",
            "   #######################  Train Epoch: 12 train_acc: 0.5441  test_acc: 0.5038 ###################       \n",
            "   #######################  Train Epoch: 13 train_acc: 0.5512  test_acc: 0.5100 ###################       \n",
            "   #######################  Train Epoch: 14 train_acc: 0.5591  test_acc: 0.5095 ###################       \n",
            "   #######################  Train Epoch: 15 train_acc: 0.5675  test_acc: 0.5240 ###################       \n",
            "   #######################  Train Epoch: 16 train_acc: 0.5756  test_acc: 0.5202 ###################       \n",
            "   #######################  Train Epoch: 17 train_acc: 0.5728  test_acc: 0.5145 ###################       \n",
            "   #######################  Train Epoch: 18 train_acc: 0.5788  test_acc: 0.5312 ###################       \n",
            "   #######################  Train Epoch: 19 train_acc: 0.5868  test_acc: 0.5212 ###################       \n",
            "   #######################  Train Epoch: 20 train_acc: 0.5965  test_acc: 0.5343 ###################       \n",
            "   #######################  Train Epoch: 21 train_acc: 0.6043  test_acc: 0.5308 ###################       \n",
            "   #######################  Train Epoch: 22 train_acc: 0.6072  test_acc: 0.5280 ###################       \n",
            "   #######################  Train Epoch: 23 train_acc: 0.6093  test_acc: 0.5225 ###################       \n",
            "   #######################  Train Epoch: 24 train_acc: 0.6102  test_acc: 0.5383 ###################       \n",
            "   #######################  Train Epoch: 25 train_acc: 0.6203  test_acc: 0.5327 ###################       \n",
            "   #######################  Train Epoch: 26 train_acc: 0.6227  test_acc: 0.5357 ###################       \n",
            "   #######################  Train Epoch: 27 train_acc: 0.6245  test_acc: 0.5335 ###################       \n",
            "   #######################  Train Epoch: 28 train_acc: 0.6309  test_acc: 0.5393 ###################       \n",
            "   #######################  Train Epoch: 29 train_acc: 0.6404  test_acc: 0.5407 ###################       \n",
            "   #######################  Train Epoch: 30 train_acc: 0.6435  test_acc: 0.5453 ###################       \n",
            "   #######################  Train Epoch: 31 train_acc: 0.6370  test_acc: 0.5390 ###################       \n",
            "   #######################  Train Epoch: 32 train_acc: 0.6511  test_acc: 0.5403 ###################       \n",
            "   #######################  Train Epoch: 33 train_acc: 0.6584  test_acc: 0.5450 ###################       \n",
            "   #######################  Train Epoch: 34 train_acc: 0.6595  test_acc: 0.5423 ###################       \n",
            "   #######################  Train Epoch: 35 train_acc: 0.6650  test_acc: 0.5427 ###################       \n",
            "   #######################  Train Epoch: 36 train_acc: 0.6715  test_acc: 0.5390 ###################       \n",
            "   #######################  Train Epoch: 37 train_acc: 0.6790  test_acc: 0.5420 ###################       \n",
            "   #######################  Train Epoch: 38 train_acc: 0.6783  test_acc: 0.5455 ###################       \n",
            "   #######################  Train Epoch: 39 train_acc: 0.6854  test_acc: 0.5370 ###################       \n",
            "   #######################  Train Epoch: 40 train_acc: 0.6992  test_acc: 0.5427 ###################       \n",
            "   #######################  Train Epoch: 41 train_acc: 0.6977  test_acc: 0.5520 ###################       \n",
            "   #######################  Train Epoch: 42 train_acc: 0.7113  test_acc: 0.5557 ###################       \n",
            "   #######################  Train Epoch: 43 train_acc: 0.7050  test_acc: 0.5423 ###################       \n",
            "   #######################  Train Epoch: 44 train_acc: 0.7089  test_acc: 0.5533 ###################       \n",
            "   #######################  Train Epoch: 45 train_acc: 0.7085  test_acc: 0.5510 ###################       \n",
            "   #######################  Train Epoch: 46 train_acc: 0.7108  test_acc: 0.5493 ###################       \n",
            "   #######################  Train Epoch: 47 train_acc: 0.7228  test_acc: 0.5545 ###################       \n",
            "   #######################  Train Epoch: 48 train_acc: 0.7266  test_acc: 0.5483 ###################       \n",
            "   #######################  Train Epoch: 49 train_acc: 0.7384  test_acc: 0.5553 ###################       \n",
            "   #######################  Train Epoch: 50 train_acc: 0.7356  test_acc: 0.5585 ###################       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's modify the learning function to make it incremental"
      ],
      "metadata": {
        "id": "ZfOlamSwmw72"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CODE\n",
        "\n",
        "First is the configuration: This is where the hyperparameters ared defined. For convenience working with Colab, it is a global dictionnary. For better practice, it should be in a file attached."
      ],
      "metadata": {
        "id": "d4RBnOZc49Mp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Facnet_config = {\n",
        "    \"Epochs\" : 10,                 # Number of epochs\n",
        "    \"learning_rate\" : 10**-4,     # learning rate\n",
        "    \"epsilon\" : 1e-8,             # epsilon to avoid 0 in denum in loss function\n",
        "    \"alpha\" : 1,                  # Margin alpha for magnet loss\n",
        "    \"nb_clusters\" : [20,20,20,20,20,20,20,20,20,20],     # Number of clusters per class\n",
        "    \"M\" : 16,                     # Number of clusters present in a mini-batch\n",
        "    \"D\" : 8,                      # Number of smaples selected from each cluster in the mini-batch\n",
        "    \"K\" : 15,                     # K for K-nearest neighbors\n",
        "    \"L\" : 128,                    # L number of nearest clusters used to predict the label of a given query (instead of KNN: magnet evaluation)\n",
        "    \"nb_batches\" : 30,\n",
        "    \"list_classes\" : [0,1,2,3,4,5,6,7,8,9],       # List of classes\n",
        "    \"batch_size\" : 32,            # mini batch size to forward the data (out of training)\n",
        "    \"optimizer_flag\" : 'Adam',    # Optimizer\n",
        "    \"width\" : 2,                  # width of wide residual block\n",
        "    \"n_increment\" : 5             # # of increments in the learning process\n",
        "}"
      ],
      "metadata": {
        "id": "vkkWnrIn7HTd"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Some functions that will help us.\n",
        "These comes from the code implementation available and are modified to adapt to the project, the environment and the dataset.\n",
        "\n",
        "**create_model**\n",
        "\n",
        "The model used is the one from the original code, dropout was not used so i added it for better results."
      ],
      "metadata": {
        "id": "xBLMrEX25374"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 2D conv layer 3x3 kernel, 1 padding, no bias.\n",
        "def conv3x3(i_c, o_c, stride=1):\n",
        "    return nn.Conv2d(i_c, o_c, 3, stride, 1, bias=False)\n",
        "\n",
        "def relu():\n",
        "    return nn.LeakyReLU(0.1)\n",
        "\n",
        "# residual block\n",
        "class residual(nn.Module):\n",
        "    def __init__(self, input_channels, output_channels, stride=1, activate_before_residual=False):\n",
        "        super().__init__()\n",
        "        layer = []\n",
        "        # If True, activation is executed first\n",
        "        if activate_before_residual:\n",
        "            self.pre_act = nn.Sequential(\n",
        "                relu()\n",
        "            )\n",
        "        else:\n",
        "\n",
        "            self.pre_act = nn.Identity()\n",
        "            layer.append(relu())\n",
        "        layer.append(conv3x3(input_channels, output_channels, stride))\n",
        "        layer.append(relu())\n",
        "        layer.append(conv3x3(output_channels, output_channels))\n",
        "\n",
        "        # Skip connections\n",
        "        if stride >= 2 or input_channels != output_channels:\n",
        "          # if dim is changing, shape is adjusted using 1x1 conv\n",
        "            self.identity = nn.Conv2d(input_channels, output_channels, 1, stride, bias=False)\n",
        "        else:\n",
        "            self.identity = nn.Identity()\n",
        "\n",
        "        self.layer = nn.Sequential(*layer)\n",
        "\n",
        "    # Apply pre-activation if needed and ads the skip connection\n",
        "    def forward(self, x):\n",
        "        x = self.pre_act(x)\n",
        "        return self.identity(x) + self.layer(x)\n",
        "\n",
        "\n",
        "class WRN2(nn.Module):\n",
        "    \"\"\" WRN28-width with leaky relu (negative slope is 0.1)\"\"\"\n",
        "    def __init__(self,width):\n",
        "        super().__init__()\n",
        "        self.init_conv = conv3x3(3, 16)\n",
        "\n",
        "        # # of filters at each stage\n",
        "        filters = [16, 16*width, 32*width, 64*width]\n",
        "\n",
        "        # first block use activation before residual\n",
        "        unit1 = [residual(filters[0], filters[1], activate_before_residual=True)] + [residual(filters[1], filters[1]) for _ in range(1, 4)]\n",
        "        self.unit1 = nn.Sequential(*unit1)\n",
        "\n",
        "        # downsampling by 2 then residual blocks\n",
        "        unit2 = [residual(filters[1], filters[2], 2)] + [residual(filters[2], filters[2]) for _ in range(1, 4)]\n",
        "        self.unit2 = nn.Sequential(*unit2)\n",
        "\n",
        "        # Downsampling by 2 again\n",
        "        unit3 = [residual(filters[2], filters[3], 2)] + [residual(filters[3], filters[3]) for _ in range(1, 4)]\n",
        "        self.unit3 = nn.Sequential(*unit3)\n",
        "\n",
        "        # final non linearity and global pooling to converts to 1x1\n",
        "        self.unit4 = nn.Sequential(*[relu(), nn.AdaptiveAvgPool2d(1)])\n",
        "        self.dropout1 = torch.nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.init_conv(x)\n",
        "        x = self.unit1(x)\n",
        "        x = self.unit2(x)\n",
        "        x = self.unit3(x)\n",
        "        x = self.unit4(x)\n",
        "\n",
        "        # Handles the case where the batch is of size 1\n",
        "        if x.shape[0]!=1:\n",
        "            x = torch.squeeze(x)\n",
        "        else:\n",
        "            x = torch.squeeze(x)\n",
        "            x = torch.unsqueeze(x, 0)\n",
        "\n",
        "        # Normalize the output vector\n",
        "        x = F.normalize(x, p=2, dim=1)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "Icie_Dee7o8s"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**forward_all_images**\n",
        "\n",
        "This function computes representations for all the imgs that are passed (i.e. the two classes for training and all the past classes for testing).\n",
        "It does it minibatch by minibatch (batch_size here is different from the batch created with class_batch). Passes all the images through the model and finally return an array containing the features."
      ],
      "metadata": {
        "id": "r52WzSuT7qiv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def forward_all_images(model,device,all_img):\n",
        "    batch_size          = Facnet_config['batch_size']\n",
        "\n",
        "    device      = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Adding this should handle both numpy array or tensors\n",
        "    # It is useful because CIFAR-10 is imported from torchvision as tensors\n",
        "    # so this step can be avoided\n",
        "    if not isinstance(all_img, torch.Tensor):\n",
        "      all_img = torch.from_numpy(all_img)\n",
        "\n",
        "    all_features  = []\n",
        "    # Iterate batch per batch\n",
        "    for i in range(0,all_img.shape[0],batch_size):\n",
        "        # Check if enough imgs remains to form a complete batch, if not, take the remaining images\n",
        "        if i+batch_size == all_img.shape[0]-1 or i+batch_size > all_img.shape[0]-1:\n",
        "            s         = i\n",
        "            data      = all_img[s:].to(device=device, dtype=torch.float)\n",
        "            features1 = model(data)\n",
        "        else:\n",
        "            s = i\n",
        "            e = s + batch_size\n",
        "\n",
        "            data      = all_img[s:e].to(device=device, dtype=torch.float)\n",
        "            features1 = model(data)\n",
        "\n",
        "        all_features.append(features1.cpu().detach().numpy())\n",
        "\n",
        "    all_features = np.concatenate(all_features,axis=0)\n",
        "\n",
        "    # Return an array containing all the features for each sample\n",
        "    return all_features\n"
      ],
      "metadata": {
        "id": "sDzYLhLR6aIV"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**cluster_training**\n",
        "\n",
        "This function extract from the features and the label a set of clusters for each class. It will be useful later to choose the samples that will form the minibatch and compute the loss. It refers to the ``nb_clusters`` hparams to know how much cluster should be created per class.\n",
        "\n",
        "For the project, it has been adapted to manage a specific set of classes (increment)\n",
        "\n",
        "It returns:\n",
        "\n",
        "``clusters_centers``: Where each line is a center of a cluster\n",
        "\n",
        "``clusters_labels``: Each row here is a [c:i] couple indicating class c and cluster i\n",
        "\n",
        "``samples_indices``: Each element is a tab of idx of all the training examples that belong to the cluster at this position\n",
        "\n",
        "``sigma``: The mean variance of all the clusters.\n"
      ],
      "metadata": {
        "id": "aFAEsCzY8boP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "def cluster_training(train_features,train_labels,increment_nb):\n",
        "\n",
        "    # Retrieve the # of clusters needed for the actual class of the increment\n",
        "    nb_clusters_list = Facnet_config['nb_clusters_train']\n",
        "    # Retrieve the class labels that will be considered\n",
        "    list_classes     = Facnet_config['list_classes_train']\n",
        "\n",
        "    clusters_labels  = []\n",
        "    samples_indices  = []\n",
        "    clusters_centers = []\n",
        "    sigma = 0\n",
        "\n",
        "    # For each class among all the one present in the increment\n",
        "    for c in list_classes:\n",
        "\n",
        "        # We retrieve the features belonging to the considered class and the # of clusters associated\n",
        "        nb_clusters          = nb_clusters_list[c-increment_nb*len(nb_clusters_list)]\n",
        "        inds_c               = np.where(train_labels==c)[0]\n",
        "        class_features       = train_features[inds_c]\n",
        "\n",
        "        # We apply KMeans with the wanted # of clusters on the features of this class and add the total inertia for this class to sigma\n",
        "        kM = KMeans(n_clusters=nb_clusters, random_state=0).fit(class_features)\n",
        "        sigma = sigma + kM.inertia_\n",
        "        # From KMeans we retrieve the cluster centers, their labels and the associated idxs of the imgs\n",
        "        for i in range(nb_clusters):\n",
        "            clusters_centers.append(kM.cluster_centers_[i])\n",
        "            clusters_labels.append(np.array([c,i]))\n",
        "            samples_indices.append(inds_c[np.where(kM.labels_==i)[0]])\n",
        "\n",
        "    clusters_centers = np.stack(clusters_centers,axis=0)\n",
        "    clusters_labels  = np.stack(clusters_labels,axis=0)\n",
        "\n",
        "    # Careful here, this assumes that nb_cluster is the same for all the classes\n",
        "    # Sigma is the mean variance for all the clusters\n",
        "    sigma = sigma/(nb_clusters*len(list_classes))\n",
        "\n",
        "    return clusters_centers,clusters_labels,samples_indices,sigma"
      ],
      "metadata": {
        "id": "MdNLvfrR8jMg"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**class_batch**\n",
        "\n",
        "cluster_training gives us a cluster structure. From it, class batch is responsible of creating the mini-batches adapted to the Magnet loss.\n",
        "\n",
        "In the ``initialization``, it retrieve the infos from cluster_training and some hyperparameters. It then construct two matrices, first P_dis is the distance b/w all the cluster centers and the second S_dis is a matrix where each row is all the idxs of the closest neighbors from each sorted according to the computed euclidean distance. The cluster itself has been removed.\n",
        "\n",
        "In the ``construct_batch`` method,"
      ],
      "metadata": {
        "id": "29xVOWnU8iEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "\n",
        "class class_batch():\n",
        "    def __init__(self, clusters_centers,clusters_labels,samples_indices):\n",
        "\n",
        "        # # of neighbors clusters that we take to form the MB\n",
        "        self.M                 = Facnet_config['M']\n",
        "        # # of samples we will use for each cluster\n",
        "        self.D                 = Facnet_config['D']\n",
        "\n",
        "        self.clusters_centers  = clusters_centers\n",
        "        self.clusters_labels   = clusters_labels\n",
        "        self.samples_indices   = samples_indices\n",
        "        self.P_dis             = euclidean_distances(clusters_centers,clusters_centers)\n",
        "        self.S_dis             = np.argsort(self.P_dis,axis=1)[:,1:]\n",
        "\n",
        "    # This function is applied for one cluster at a time (seed). inds1 represents the cluster position b/w all the cluster in the increment.\n",
        "    # samples_indices has the same idxing where each idx is an array of all the indices of the samples belonging to the cluster at pos inds1\n",
        "    def construct_batch(self,inds1):\n",
        "        batch_inds  = []\n",
        "        batch_label = []\n",
        "\n",
        "        # If the seed cluster has less than D samples\n",
        "        if self.samples_indices[inds1].shape[0]<self.D:\n",
        "            # Take all the sample of the seed cluster (label is also retrieved and repeated D times)\n",
        "            batch_inds.append(self.samples_indices[inds1])\n",
        "            batch_label.append(np.repeat(np.expand_dims(self.clusters_labels[inds1],axis=0),self.samples_indices[inds1].shape[0],axis=0))\n",
        "        else:\n",
        "            # Take D random samples of the seed cluster (label is also retrieved and repeated D times)\n",
        "            batch_inds.append(self.samples_indices[inds1][np.random.randint(low=0, high=self.samples_indices[inds1].shape[0], size=self.D)])\n",
        "            batch_label.append(np.repeat(np.expand_dims(self.clusters_labels[inds1],axis=0),self.D,axis=0))\n",
        "\n",
        "        # Determines which are the M closest centroids using  the S_dis matrix\n",
        "        clusters_inds = self.choose_nearest(inds1)\n",
        "\n",
        "        # For each of the chosen closest clusters we also retrieve D samples and labels\n",
        "        for i in range(clusters_inds.shape[0]):\n",
        "            inds2 = clusters_inds[i]\n",
        "            if self.samples_indices[inds2].shape[0]<self.D:\n",
        "                batch_inds.append(self.samples_indices[inds2])\n",
        "                batch_label.append(np.repeat(np.expand_dims(self.clusters_labels[inds2],axis=0),self.samples_indices[inds2].shape[0],axis=0))\n",
        "            else:\n",
        "                batch_inds.append(self.samples_indices[inds2][np.random.randint(low=0, high=self.samples_indices[inds2].shape[0], size=self.D)])\n",
        "                batch_label.append(np.repeat(np.expand_dims(self.clusters_labels[inds2],axis=0),self.D,axis=0))\n",
        "\n",
        "        batch_inds      = np.concatenate(batch_inds,axis=0)\n",
        "        batch_label     = np.concatenate(batch_label,axis=0)\n",
        "        chosen_clusters = np.unique(batch_label, axis=0)\n",
        "\n",
        "        # Returns in form of arrays\n",
        "        return batch_inds,batch_label,chosen_clusters\n",
        "\n",
        "    def choose_nearest(self,inds1):\n",
        "\n",
        "        pos = int((2/3)*self.M)\n",
        "        neg = int((1/3)*self.M)\n",
        "\n",
        "        labels  = self.clusters_labels[self.S_dis[inds1,:],0]\n",
        "        test    = np.where(labels[:self.M]!=self.clusters_labels[inds1,0])[0].shape[0]\n",
        "\n",
        "        # We make sure the M closest neighbors are at least composed of 1/3 of different classes\n",
        "        if test>neg:\n",
        "            clusters_inds = self.S_dis[inds1,:self.M]\n",
        "        else:\n",
        "            inds_eq   = np.where(labels==self.clusters_labels[inds1,0])[0]\n",
        "            inds_ineq = np.where(labels!=self.clusters_labels[inds1,0])[0]\n",
        "\n",
        "            clusters_inds = np.concatenate([inds_eq[:pos],inds_ineq[:neg]],axis=0)\n",
        "\n",
        "        return clusters_inds\n",
        "\n"
      ],
      "metadata": {
        "id": "X8O2tFmY87tB"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**test_model**\n",
        "\n",
        "The role of test model is to evaluate the actual representations."
      ],
      "metadata": {
        "id": "V2RKU_1k8bUX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def test_model(train_features, labels_train,test_features,labels_test, train=False):\n",
        "\n",
        "    # Retrieve the # of neighbors to consider\n",
        "    K                   = Facnet_config['K']\n",
        "\n",
        "    # Compute squared distances b/w train and test features\n",
        "    D  = euclidean_distances(test_features,train_features)\n",
        "    D  = np.square(D)\n",
        "\n",
        "    # I here is a matrix where each row is the idx of the closest train sample for each test example (closest first)\n",
        "    I  = np.argsort(D,axis=1)\n",
        "\n",
        "    # Compute the confidence vector with only the considered neighbors (removing itself in case of training evaluation I guess)\n",
        "    Conf1     = KNN_conf(I[:,1:K+1], labels_train, train)\n",
        "    # Looks useless\n",
        "    Conf      = np.max(Conf1,axis=1)\n",
        "\n",
        "    # Prediction and accuracy\n",
        "    label_pred = np.argmax(Conf1,axis=1)\n",
        "    acc = accuracy_score(labels_test, label_pred)\n",
        "\n",
        "    return label_pred,acc\n",
        "\n",
        "\n",
        "def KNN_conf(mat, labels_train, train):\n",
        "\n",
        "    # We evaluate on all the classes, as the model doesn't have any samples belonging to previous classes it will simply have a 0 confidence\n",
        "    # on them so it doesn't bother us\n",
        "    conf_vector1 = np.zeros((mat.shape[0],len(Facnet_config['list_classes_test'])))\n",
        "    for i in range(mat.shape[0]):\n",
        "        for c,j in enumerate(Facnet_config['list_classes_test']):\n",
        "            conf_vector1[i,c] = np.where(labels_train[mat[i,:]]==j)[0].shape[0]/mat.shape[1]\n",
        "\n",
        "    return conf_vector1"
      ],
      "metadata": {
        "id": "8AjjQSor9JDn"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**resize_data**\n",
        "\n",
        "These are helper functions for the task, it allows to form the training set and test set for each increment"
      ],
      "metadata": {
        "id": "AldXN3XG9UwB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def resize_dataset(dataset, n_samples, n_cls=10):\n",
        "    \"\"\"This function aims to resize the CIFAR-10 dataset with a given number\n",
        "    of samples per class.\n",
        "    It assumes that the labels are digits from 0 to n_class\"\"\"\n",
        "    imgs_per_class = [[] for i in range(n_cls)]\n",
        "    labels_per_class = [[] for i in range(n_cls)]\n",
        "\n",
        "    for img, label in dataset:\n",
        "        if(len(imgs_per_class[label]) < n_samples):\n",
        "            imgs_per_class[label].append(img)\n",
        "            labels_per_class[label].append(label)\n",
        "\n",
        "    imgs_per_class = [torch.stack(imgs) for imgs in imgs_per_class]\n",
        "    labels_per_class = [torch.tensor(labels) for labels in labels_per_class]\n",
        "\n",
        "    images_set = torch.cat(imgs_per_class)\n",
        "    labels_set = torch.cat(labels_per_class)\n",
        "\n",
        "    return images_set, labels_set\n",
        "\n",
        "def split_train_data(train_data, train_labels, n_increments):\n",
        "    \"\"\"This function is used to split the training data into equally sized increments regarding the # of classes in it.\n",
        "        It returns a list with all train set corresponding to the increments\"\"\"\n",
        "    train_data_per_increment = []\n",
        "    train_label_per_increment = []\n",
        "    n_cls = len(torch.unique(train_labels))\n",
        "    n_class_per_increment = n_cls // n_increments\n",
        "\n",
        "    # Make sure the increment number can divide properly the total number of classes\n",
        "    if n_cls % n_increments != 0:\n",
        "        raise ValueError(\"n_increments should divide the number of classes in the dataset\")\n",
        "\n",
        "    # Assuming class labels goes from 0 to n_classes\n",
        "    # Retrieve the portion of the main set an labels that belongs to the considered classes of the increment\n",
        "    for incr in range(n_increments):\n",
        "        cls_idx = torch.tensor([i+incr*n_class_per_increment for i in range(n_class_per_increment)])\n",
        "        incr_idx = torch.isin(train_labels, cls_idx)\n",
        "        train_label_per_increment.append(train_labels[incr_idx])\n",
        "        train_data_per_increment.append(train_data[incr_idx])\n",
        "\n",
        "    return train_data_per_increment, train_label_per_increment\n",
        "\n",
        "def split_test_data(test_data, test_labels, n_increments):\n",
        "  \"\"\"This function is used to split the test data into set of increasing size gathering all the past data.\n",
        "      It returns two list of data and labels associated with each increment\"\"\"\n",
        "  test_data_per_increment = []\n",
        "  test_label_per_increment = []\n",
        "\n",
        "  n_cls = len(torch.unique(test_labels))\n",
        "  n_class_per_increment = n_cls // n_increments\n",
        "\n",
        "  if n_cls % n_increments != 0:\n",
        "        raise ValueError(\"n_increments should divide the number of classes in the dataset\")\n",
        "\n",
        "  for incr in range(1, n_increments+1):\n",
        "    cls_idx = torch.tensor([i for i in range(n_class_per_increment * incr)])\n",
        "    incr_idx = torch.isin(test_labels, cls_idx)\n",
        "    test_label_per_increment.append(test_labels[incr_idx])\n",
        "    test_data_per_increment.append(test_data[incr_idx])\n",
        "\n",
        "  return test_data_per_increment, test_label_per_increment\n"
      ],
      "metadata": {
        "id": "V_EtYbkS9YU3"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**loss_function**\n",
        "\n",
        "This class computes the magnet loss for the current batch"
      ],
      "metadata": {
        "id": "_p1pjmz29iux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.autograd import Function\n",
        "\n",
        "class loss_function(Function):\n",
        "    def __init__(self, alpha):\n",
        "        self.alpha  = alpha\n",
        "        self.M      = Facnet_config['M']\n",
        "        self.D      = Facnet_config['D']\n",
        "        self.pdist  = torch.nn.PairwiseDistance(p=2)\n",
        "\n",
        "        self.epsilon      = Facnet_config['epsilon']\n",
        "\n",
        "    # Compute the mean per cluster and the averaged dispersion per cluster for the current minibatch\n",
        "    def mean_sigma(self, output, batch_label,chosen_clusters,device):\n",
        "\n",
        "        sigma         = torch.zeros(chosen_clusters.shape[0]).to(device)\n",
        "        mean_clusters = torch.zeros((chosen_clusters.shape[0],output.shape[1])).to(device)\n",
        "        for i in range(chosen_clusters.shape[0]):\n",
        "            inds               = torch.where((batch_label[:,0]==chosen_clusters[i,0]) & (batch_label[:,1]==chosen_clusters[i,1]) )[0]\n",
        "            mean_clusters[i,:] = output[inds].mean(0)\n",
        "            sigma[i]           = self.pdist(output[inds].mean(0),output[inds]).pow(2).mean(0)\n",
        "\n",
        "        sigma = sigma.mean(0)\n",
        "        return sigma, mean_clusters\n",
        "\n",
        "    # Computes the magnet loss for the given mb\n",
        "    def forward(self, output, batch_label,chosen_clusters,device):\n",
        "\n",
        "        sigma, mean_clusters = self.mean_sigma(output, batch_label,chosen_clusters,device)\n",
        "        loss1 = torch.zeros(output.shape[0]).to(device)\n",
        "\n",
        "        # loss contribution of each example in the minibatch\n",
        "        for s in range(output.shape[0]):\n",
        "            # negative scaled squared distance\n",
        "            dis        = -(1/(2*sigma.pow(2)))*self.pdist(output[s],mean_clusters).pow(2)\n",
        "            # retrieve idx that matches the seed\n",
        "            inds1      = torch.where((chosen_clusters[:,0]==batch_label[s,0]) & (chosen_clusters[:,1]==batch_label[s,1]) )[0]\n",
        "            # others\n",
        "            inds2      = torch.where(chosen_clusters[:,0]!=batch_label[s,0])[0]\n",
        "            # apply the alpha to the distance to itself\n",
        "            num        = torch.exp(dis[inds1]-self.alpha)\n",
        "            # sum of exp of all the other distances\n",
        "            den        = torch.exp(dis[inds2]).sum(0)\n",
        "            # final loss\n",
        "            loss1[s]   = -torch.log(num/(den+self.epsilon) + self.epsilon)\n",
        "\n",
        "        # make sure positive\n",
        "        loss2 = torch.clamp(loss1, min = 0.0)\n",
        "        # overall loss\n",
        "        loss  = loss2.mean(0)\n",
        "\n",
        "        return loss\n"
      ],
      "metadata": {
        "id": "zumv80GH9mTO"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**incremental_learning_function**\n",
        "\n",
        "This implements the training logic of the model. It does it by increments based on the hparams defined by the user."
      ],
      "metadata": {
        "id": "FC1YvQAScuVI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from forward_all_images import forward_all_images\n",
        "#from loss_function import loss_function\n",
        "#from test_model import test_model\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "#from config import Facnet_config\n",
        "#from cluster_training import cluster_training\n",
        "#from class_batch import class_batch\n",
        "\n",
        "def incremental_learning_function(model,optimizer,device,train_data_per_increment, train_label_per_increment,test_data_per_increment,test_label_per_increment, n_increment):\n",
        "\n",
        "\n",
        "    ''' #################################################  initialization  ################################################### '''\n",
        "    Epochs              = Facnet_config['Epochs']\n",
        "    nb_batches          = Facnet_config['nb_batches']\n",
        "    alpha               = Facnet_config['alpha']\n",
        "    list_cls = Facnet_config['list_classes']\n",
        "    nb_clstr = Facnet_config['nb_clusters']\n",
        "\n",
        "    train_acc_increment, test_acc_increment = [], []\n",
        "\n",
        "    # iterate over each increment\n",
        "    for increment in range(n_increment):\n",
        "\n",
        "      # # of classes in one increment\n",
        "      increment_size = len(list_cls) // n_increment\n",
        "\n",
        "      # Defines the new classes that are observed by the model at each increment for training hiding previous one\n",
        "      list_cls_train_incr = list_cls[increment_size*increment:increment_size*(increment+1)]\n",
        "      # Defines the new classes that are observed by the model at each increment for testing keeping previous one\n",
        "      list_cls_test_incr = list_cls[0:increment_size*(increment+1)]\n",
        "      Facnet_config['list_classes_train'] = list_cls_train_incr\n",
        "      Facnet_config['list_classes_test'] = list_cls_test_incr\n",
        "\n",
        "      # Retrieve # of clusters for the considered class\n",
        "      nb_cluster_train_incr = nb_clstr[increment_size*increment:increment_size*(increment+1)]\n",
        "      Facnet_config['nb_clusters_train'] = nb_cluster_train_incr\n",
        "\n",
        "      train_acc, test_acc, loss = [], [], []\n",
        "\n",
        "      print(\"----------------------\\n\"\n",
        "      +f\"Starting training increment {increment+1} on classes {list_cls_train_incr} and testing on classes {list_cls_test_incr} ...\\n\"\n",
        "      +\"----------------------\")\n",
        "      # Retrieve the test/train data and labels associated to this increment\n",
        "      images_train, labels_train = train_data_per_increment[increment], train_label_per_increment[increment]\n",
        "      images_test, labels_test = test_data_per_increment[increment], test_label_per_increment[increment]\n",
        "\n",
        "      ''' #################################################  test the initial model  ################################################### '''\n",
        "      # Compute features\n",
        "      train_features = forward_all_images(model,device,images_train)\n",
        "      test_features  = forward_all_images(model,device,images_test)\n",
        "\n",
        "      # Use KMeans and compute the cluster centers, labels, idx associated with each and the mean variance sigma\n",
        "      clusters_centers,clusters_labels,samples_indices,sigma = cluster_training(train_features,labels_train, increment_nb=increment)\n",
        "      # Create a mini batch object from the info of the clustering\n",
        "      CB      = class_batch(clusters_centers,clusters_labels,samples_indices)\n",
        "\n",
        "      # Evaluate the model first without training\n",
        "      _,train_acc_1  = test_model(train_features, labels_train,train_features,labels_train, train=True)\n",
        "      _,test_acc_1   = test_model(train_features, labels_train,test_features,labels_test, train=False)\n",
        "\n",
        "      train_acc.append(train_acc_1)\n",
        "      test_acc.append(test_acc_1)\n",
        "\n",
        "      torch.save(model.state_dict(), r'models/model'+str(0)+'.pth')\n",
        "      print(\"   #######################  Train Epoch: {} train_acc: {:0.4f}  test_acc: {:0.4f} ###################       \".format(0,train_acc[-1],test_acc[-1]))\n",
        "\n",
        "      ''' #################################################### block for learning  ########################################################## '''\n",
        "      for i in range(1,Epochs+1):\n",
        "\n",
        "          '''##################################################### Learning ################################################################################'''\n",
        "\n",
        "\n",
        "          indices = np.arange(clusters_centers.shape[0])\n",
        "          # For each cluster\n",
        "          for s in range(indices.shape[0]):\n",
        "            # we create a mini_batch and retrieve the idxs of the chosen examples, labels, and neigbors clusters\n",
        "              batch_inds,batch_label,chosen_clusters = CB.construct_batch(indices[s])\n",
        "              targets                = images_train[batch_inds]\n",
        "\n",
        "              # Once again, I just modified to handle tensors\n",
        "              if isinstance(targets, torch.Tensor):\n",
        "                targets = targets.to(device=device, dtype=torch.float)\n",
        "              else:\n",
        "                targets         = torch.from_numpy(targets).to(device=device, dtype=torch.float)\n",
        "\n",
        "              batch_label     = torch.from_numpy(batch_label).to(device=device, dtype=torch.float)\n",
        "              chosen_clusters = torch.from_numpy(chosen_clusters).to(device=device, dtype=torch.float)\n",
        "\n",
        "              # Train and update the model\n",
        "              model.train()\n",
        "              output  = model(targets)\n",
        "              loss   = loss_function(alpha).forward(output, batch_label,chosen_clusters,device).to(device)\n",
        "\n",
        "              optimizer.zero_grad()\n",
        "              loss.backward()\n",
        "              optimizer.step()\n",
        "\n",
        "\n",
        "          '''######################################################## Forward the data  ########################################################################'''\n",
        "          # After an update for each cluster we pass the imgs again through the model\n",
        "          train_features = forward_all_images(model,device,images_train)\n",
        "          test_features  = forward_all_images(model,device,images_test)\n",
        "\n",
        "          '''########################################################## Clustering  ##########################################################################'''\n",
        "          # Defined the structure again using KMeans\n",
        "          clusters_centers,clusters_labels,samples_indices,sigma = cluster_training(train_features,labels_train, increment_nb=increment)\n",
        "          CB      = class_batch(clusters_centers,clusters_labels,samples_indices)\n",
        "\n",
        "          '''##################################################### testing with KNN ##########################################################################'''\n",
        "          # Re-evaluate\n",
        "          _,train_acc_1  = test_model(train_features, labels_train,train_features,labels_train, train=True)\n",
        "          _,test_acc_1   = test_model(train_features, labels_train,test_features,labels_test, train=False)\n",
        "\n",
        "          '''###################################################################################################################################################'''\n",
        "          train_acc.append(train_acc_1)\n",
        "          test_acc.append(test_acc_1)\n",
        "\n",
        "          torch.save(model.state_dict(), r'models/model'+str(i)+'.pth')\n",
        "          print(\"   #######################  Train Epoch: {} train_acc: {:0.4f}  test_acc: {:0.4f} ###################       \".format(i,train_acc[-1],test_acc[-1]))\n",
        "\n",
        "      train_acc = np.stack(train_acc)\n",
        "      test_acc  = np.stack(test_acc)\n",
        "\n",
        "      train_acc_increment.append(train_acc[-1])\n",
        "      test_acc_increment.append(test_acc[-1])\n",
        "\n",
        "      print(f\"Final Accs for increment {increment+1}: Train Acc: {train_acc_increment[increment]:0.4f} Test Acc: {test_acc_increment[increment]:0.4f}\")\n",
        "\n",
        "      np.save(r'results/train_acc',train_acc)\n",
        "      np.save(r'results/test_acc',test_acc)\n",
        "\n",
        "    train_acc_increment = np.stack(train_acc_increment)\n",
        "    test_acc_increment  = np.stack(test_acc_increment)\n",
        "\n",
        "    np.save(r'results/train_acc_per_increment',train_acc_increment)\n",
        "    np.save(r'results/test_acc_per_increment',test_acc_increment)"
      ],
      "metadata": {
        "id": "ox7LyG2iQUEJ"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**main**"
      ],
      "metadata": {
        "id": "F1Dh3sCqnSsH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "torch.manual_seed(3)\n",
        "#from resize_data import resize_dataset, split_train_data, split_test_data\n",
        "\n",
        "#import create_model\n",
        "#from learning_function import learning_function\n",
        "#from config import Facnet_config\n",
        "\n",
        "if not os.path.exists(\"models\"):\n",
        "    os.makedirs(\"models\")\n",
        "if not os.path.exists(\"results\"):\n",
        "    os.makedirs(\"results\")\n",
        "\n",
        "print(\"Importing data...\")\n",
        "# Define a transform (here no resizing is needed for 32x32 images)\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Download and load the CIFAR-10 training and test datasets\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset  = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Use 500 samples per class for learning\n",
        "images_train, labels_train = resize_dataset(train_dataset, 500, 10)\n",
        "images_test, labels_test   = resize_dataset(test_dataset, 100, 10)\n",
        "\n",
        "print(f\"images_train size: {images_train.shape}\")\n",
        "print(f\"labels_train size: {labels_train.shape}\")\n",
        "print(f\"images_test size: {images_test.shape}\")\n",
        "print(f\"labels_test size: {labels_test.shape}\")\n",
        "\n",
        "learning_rate       = Facnet_config['learning_rate']\n",
        "width               = Facnet_config['width']\n",
        "n_increment         = Facnet_config['n_increment']\n",
        "\n",
        "# Split our train/test data in 5 increments\n",
        "train_data_per_increment, train_label_per_increment = split_train_data(images_train, labels_train, n_increment)\n",
        "test_data_per_increment, test_label_per_increment = split_test_data(images_test, labels_test, n_increment)\n",
        "\n",
        "# Create the model\n",
        "model     = WRN2(width)\n",
        "device    = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model     = model.to(device=device, dtype=torch.float)\n",
        "model.eval()\n",
        "\n",
        "# Define the optimizer and start the incremental learning process\n",
        "optimizer     = torch.optim.Adam(model.parameters(),lr = learning_rate)\n",
        "incremental_learning_function(model,optimizer,device,train_data_per_increment, train_label_per_increment,test_data_per_increment,test_label_per_increment,n_increment)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qs5BK5YfnR79",
        "outputId": "3413e579-4c51-4620-a41c-34aa4938bf41"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Importing data...\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:05<00:00, 28.7MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "images_train size: torch.Size([5000, 3, 32, 32])\n",
            "labels_train size: torch.Size([5000])\n",
            "images_test size: torch.Size([1000, 3, 32, 32])\n",
            "labels_test size: torch.Size([1000])\n",
            "----------------------\n",
            "Starting training increment 1 on classes [0, 1] and testing on classes [0, 1] ...\n",
            "----------------------\n",
            "   #######################  Train Epoch: 0 train_acc: 0.7830  test_acc: 0.7100 ###################       \n",
            "   #######################  Train Epoch: 1 train_acc: 0.8010  test_acc: 0.7700 ###################       \n",
            "   #######################  Train Epoch: 2 train_acc: 0.8170  test_acc: 0.7900 ###################       \n",
            "   #######################  Train Epoch: 3 train_acc: 0.8360  test_acc: 0.8050 ###################       \n",
            "   #######################  Train Epoch: 4 train_acc: 0.8470  test_acc: 0.8100 ###################       \n",
            "   #######################  Train Epoch: 5 train_acc: 0.8910  test_acc: 0.8350 ###################       \n",
            "   #######################  Train Epoch: 6 train_acc: 0.9110  test_acc: 0.8350 ###################       \n",
            "   #######################  Train Epoch: 7 train_acc: 0.9330  test_acc: 0.8350 ###################       \n",
            "   #######################  Train Epoch: 8 train_acc: 0.9550  test_acc: 0.8500 ###################       \n",
            "   #######################  Train Epoch: 9 train_acc: 0.9310  test_acc: 0.8250 ###################       \n",
            "   #######################  Train Epoch: 10 train_acc: 0.9580  test_acc: 0.8400 ###################       \n",
            "Final Accs for increment 1: Train Acc: 0.9580 Test Acc: 0.8400\n",
            "----------------------\n",
            "Starting training increment 2 on classes [2, 3] and testing on classes [0, 1, 2, 3] ...\n",
            "----------------------\n",
            "   #######################  Train Epoch: 0 train_acc: 0.7170  test_acc: 0.3350 ###################       \n",
            "   #######################  Train Epoch: 1 train_acc: 0.7510  test_acc: 0.3275 ###################       \n",
            "   #######################  Train Epoch: 2 train_acc: 0.7850  test_acc: 0.3450 ###################       \n",
            "   #######################  Train Epoch: 3 train_acc: 0.8180  test_acc: 0.3425 ###################       \n",
            "   #######################  Train Epoch: 4 train_acc: 0.8240  test_acc: 0.3525 ###################       \n",
            "   #######################  Train Epoch: 5 train_acc: 0.8470  test_acc: 0.3625 ###################       \n",
            "   #######################  Train Epoch: 6 train_acc: 0.8640  test_acc: 0.3375 ###################       \n",
            "   #######################  Train Epoch: 7 train_acc: 0.8920  test_acc: 0.3400 ###################       \n",
            "   #######################  Train Epoch: 8 train_acc: 0.9050  test_acc: 0.3575 ###################       \n",
            "   #######################  Train Epoch: 9 train_acc: 0.8770  test_acc: 0.3500 ###################       \n",
            "   #######################  Train Epoch: 10 train_acc: 0.9110  test_acc: 0.3550 ###################       \n",
            "Final Accs for increment 2: Train Acc: 0.9110 Test Acc: 0.3550\n",
            "----------------------\n",
            "Starting training increment 3 on classes [4, 5] and testing on classes [0, 1, 2, 3, 4, 5] ...\n",
            "----------------------\n",
            "   #######################  Train Epoch: 0 train_acc: 0.7310  test_acc: 0.2317 ###################       \n",
            "   #######################  Train Epoch: 1 train_acc: 0.7760  test_acc: 0.2450 ###################       \n",
            "   #######################  Train Epoch: 2 train_acc: 0.8110  test_acc: 0.2600 ###################       \n",
            "   #######################  Train Epoch: 3 train_acc: 0.8360  test_acc: 0.2550 ###################       \n",
            "   #######################  Train Epoch: 4 train_acc: 0.8640  test_acc: 0.2567 ###################       \n",
            "   #######################  Train Epoch: 5 train_acc: 0.8850  test_acc: 0.2667 ###################       \n",
            "   #######################  Train Epoch: 6 train_acc: 0.9100  test_acc: 0.2600 ###################       \n",
            "   #######################  Train Epoch: 7 train_acc: 0.9160  test_acc: 0.2667 ###################       \n",
            "   #######################  Train Epoch: 8 train_acc: 0.9260  test_acc: 0.2550 ###################       \n",
            "   #######################  Train Epoch: 9 train_acc: 0.9410  test_acc: 0.2633 ###################       \n",
            "   #######################  Train Epoch: 10 train_acc: 0.9310  test_acc: 0.2633 ###################       \n",
            "Final Accs for increment 3: Train Acc: 0.9310 Test Acc: 0.2633\n",
            "----------------------\n",
            "Starting training increment 4 on classes [6, 7] and testing on classes [0, 1, 2, 3, 4, 5, 6, 7] ...\n",
            "----------------------\n",
            "   #######################  Train Epoch: 0 train_acc: 0.7430  test_acc: 0.1888 ###################       \n",
            "   #######################  Train Epoch: 1 train_acc: 0.8600  test_acc: 0.1900 ###################       \n",
            "   #######################  Train Epoch: 2 train_acc: 0.8980  test_acc: 0.2050 ###################       \n",
            "   #######################  Train Epoch: 3 train_acc: 0.9150  test_acc: 0.2062 ###################       \n",
            "   #######################  Train Epoch: 4 train_acc: 0.9380  test_acc: 0.2125 ###################       \n",
            "   #######################  Train Epoch: 5 train_acc: 0.9430  test_acc: 0.2125 ###################       \n",
            "   #######################  Train Epoch: 6 train_acc: 0.9490  test_acc: 0.2213 ###################       \n",
            "   #######################  Train Epoch: 7 train_acc: 0.9580  test_acc: 0.2175 ###################       \n",
            "   #######################  Train Epoch: 8 train_acc: 0.9730  test_acc: 0.2162 ###################       \n",
            "   #######################  Train Epoch: 9 train_acc: 0.9730  test_acc: 0.2200 ###################       \n",
            "   #######################  Train Epoch: 10 train_acc: 0.9770  test_acc: 0.2225 ###################       \n",
            "Final Accs for increment 4: Train Acc: 0.9770 Test Acc: 0.2225\n",
            "----------------------\n",
            "Starting training increment 5 on classes [8, 9] and testing on classes [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] ...\n",
            "----------------------\n",
            "   #######################  Train Epoch: 0 train_acc: 0.8070  test_acc: 0.1560 ###################       \n",
            "   #######################  Train Epoch: 1 train_acc: 0.8640  test_acc: 0.1610 ###################       \n",
            "   #######################  Train Epoch: 2 train_acc: 0.8820  test_acc: 0.1590 ###################       \n",
            "   #######################  Train Epoch: 3 train_acc: 0.9150  test_acc: 0.1640 ###################       \n",
            "   #######################  Train Epoch: 4 train_acc: 0.9290  test_acc: 0.1670 ###################       \n",
            "   #######################  Train Epoch: 5 train_acc: 0.9260  test_acc: 0.1670 ###################       \n",
            "   #######################  Train Epoch: 6 train_acc: 0.9440  test_acc: 0.1670 ###################       \n",
            "   #######################  Train Epoch: 7 train_acc: 0.9530  test_acc: 0.1640 ###################       \n",
            "   #######################  Train Epoch: 8 train_acc: 0.9670  test_acc: 0.1650 ###################       \n",
            "   #######################  Train Epoch: 9 train_acc: 0.9600  test_acc: 0.1650 ###################       \n",
            "   #######################  Train Epoch: 10 train_acc: 0.9550  test_acc: 0.1630 ###################       \n",
            "Final Accs for increment 5: Train Acc: 0.9550 Test Acc: 0.1630\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Final notes\n",
        "\n",
        "These results shows that the model is catastrophically forgetting the previous classes, the test accuracy is going down which is expected while the training accuracy increases benefiting slightly from previous training sessions.\n",
        "\n",
        "However, the results are not impressive, it should be linked to the used model that is overfitting especially highlighted on the regular training. Dedicating more time to try other arcitecture would be very interesting but I wanted to get this done quickly (my courses this semester are very demanding) and I am sorry for that. I would love to submit something with more experimental variations.\n",
        "\n",
        "Hope it will be fine for you,\n",
        "\n",
        "Best,\n",
        "\n",
        "Sam"
      ],
      "metadata": {
        "id": "YOKT6pEedG_4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vCvN7li4dJyq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}